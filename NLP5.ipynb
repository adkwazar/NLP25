{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Przewidywanie kategorii\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Przygotowanie danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapisanie receznji do listy krootek ([slowa], 'pos'/'neg')\n",
    "documents = [] \n",
    "for category in movie_reviews.categories(): \n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((list(movie_reviews.words(fileid)), category))\n",
    "\n",
    "#tasowanie recenzji\n",
    "random.shuffle(documents)\n",
    "\n",
    "#wszystkie slowa z recenzji\n",
    "all_words = [] \n",
    "for w in movie_reviews.words():  \n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "#zliczanie częstosci występowania tych slow\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "#3000 najpopularniejszych slow\n",
    "word_features = [x[0] for x in all_words.most_common(3000)]\n",
    "\n",
    "\n",
    "#Funkcja zwracająca slownik, gdzie klucze to word_features, a klucze to True lub False w zależnosci od tego czy dane slowo występuje w dokumencie czy nie\n",
    "def find_features(document): \n",
    "    words = set(document)    \n",
    "    features = {}            \n",
    "    for w in word_features: \n",
    "        features[w] = (w in words) \n",
    "    return features\n",
    "\n",
    "#wywoluje te funkcje dla wszystkich dokumentow, dodatkowo zapisuje do krotki informacje o kategorii recenzji\n",
    "featuresets = [(find_features(rev),category) for (rev,category) in documents]\n",
    "\n",
    "#Dane do trenowania i testowania modelu\n",
    "training_set = featuresets[:1900] \n",
    "testing_set = featuresets[1900:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność metody Naive Bayes do problemu klasyfikacji na zbiorze testowym wynosi: 85.0\n"
     ]
    }
   ],
   "source": [
    "NB_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Dokładność metody Naive Bayes do problemu klasyfikacji na zbiorze testowym wynosi:\", (nltk.classify.accuracy(NB_classifier,testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność metody MNB Naive Bayes do problemu klasyfikacji na zbiorze testowym wynosi: 85.0\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"Dokładność metody MNB Naive Bayes do problemu klasyfikacji na zbiorze testowym wynosi:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność metody Bernoulli Naive Bayes do problemu klasyfikacji na zbiorze testowym wynosi: 84.0\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"Dokładność metody Bernoulli Naive Bayes do problemu klasyfikacji na zbiorze testowym wynosi:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Regresja logistyczna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność metody Logistic Regression do problemu klasyfikacji na zbiorze testowym wynosi: 85.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"Dokładność metody Logistic Regression do problemu klasyfikacji na zbiorze testowym wynosi:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność metody Linear SVC do problemu klasyfikacji na zbiorze testowym wynosi: 85.0\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"Dokładność metody Linear SVC do problemu klasyfikacji na zbiorze testowym wynosi:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność metody SGDC do problemu klasyfikacji na zbiorze testowym wynosi: 81.0\n"
     ]
    }
   ],
   "source": [
    "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDC_classifier.train(training_set)\n",
    "print(\"Dokładność metody SGDC do problemu klasyfikacji na zbiorze testowym wynosi:\",nltk.classify.accuracy(SGDC_classifier, testing_set)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Klasyfikator zbiorczy oparty o wiele klasyfikatorów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "\n",
    "class AggClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_classifier = AggClassifier(MNB_classifier,BernoulliNB_classifier,LogisticRegression_classifier, LinearSVC_classifier,SGDC_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność zagregowanego klasyfikatora do problemu klasyfikacji na zbiorze testowym wynosi: 86.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Dokładność zagregowanego klasyfikatora do problemu klasyfikacji na zbiorze testowym wynosi:\", (nltk.classify.accuracy(agg_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisuje ten model do pliku\n",
    "\n",
    "import pickle\n",
    "save_list = open(\"my_model\",\"wb\") #w-write, b -bity\n",
    "pickle.dump(agg_classifier, save_list)\n",
    "save_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#otwieranie\n",
    "f = open(\"my_model\",\"rb\")\n",
    "my_classifier = pickle.load(f) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'a', 'and', 'to', 'is', 'it', 'this', 'i', 'one', 'at', 'was', 'all', 'like', 'when', 'which', 'would', 'well', 'first', 'after', 'little', 'me', 'my', 'movies', 'old', 'years', 'again', 'day', 'lot', 'watch', 'feel', 'book', 'classic', 'five', 'ago', 'kid', 'harry', 'gave', '7', 'started', 'reading', 'watched', 'mom', '12']\n"
     ]
    }
   ],
   "source": [
    "#przykladowa recenzja \n",
    "ex_r = \"I watched all Harry Potter movies when I was 7 years old. After five years I watched the movies again at 12 years old and I feel this saga a classic, well, it is a classic. I started reading the first book like one day ago. Which my mom gave to me when I was a little kid. I like Harry Potter a lot and I would watch all the saga again\"\n",
    "ex_r = word_tokenize(ex_r) #przerabiam tekst na slowa\n",
    "ex_r = [x.lower() for x in ex_r] #na male litery\n",
    "ex_r = find_features(ex_r)  #przerabiam recenzje na słownik: slowa: True/False\n",
    "\n",
    "\n",
    "#sprawdzam ktore slowa ze zbioru uczącego wystąpily tez w tej recenzji\n",
    "which_True =[]\n",
    "for word, b in ex_r.items():\n",
    "    if b == True:\n",
    "        which_True.append(word)\n",
    "        \n",
    "print(which_True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupa: pos | Pewność %: 60.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Grupa:\", my_classifier.classify(ex_r), \"| Pewność %:\", my_classifier.confidence(ex_r)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> N-gramy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text = \"This is an example paper in which I summed up some basic concepts reffering to viruses.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'is'), ('is', 'an'), ('an', 'example'), ('example', 'paper'), ('paper', 'in'), ('in', 'which'), ('which', 'I'), ('I', 'summed'), ('summed', 'up'), ('up', 'some'), ('some', 'basic'), ('basic', 'concepts'), ('concepts', 'reffering'), ('reffering', 'to'), ('to', 'viruses'), ('viruses', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(word_tokenize(ex_text),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'is', 'an'), ('is', 'an', 'example'), ('an', 'example', 'paper'), ('example', 'paper', 'in'), ('paper', 'in', 'which'), ('in', 'which', 'I'), ('which', 'I', 'summed'), ('I', 'summed', 'up'), ('summed', 'up', 'some'), ('up', 'some', 'basic'), ('some', 'basic', 'concepts'), ('basic', 'concepts', 'reffering'), ('concepts', 'reffering', 'to'), ('reffering', 'to', 'viruses'), ('to', 'viruses', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(word_tokenize(ex_text),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words =  set(stopwords.words(\"english\"))\n",
    "Trump = [w.lower() for w in inaugural.words('2025-Trump.txt') if w not in stop_words and w not in string.punctuation] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1629, 1619)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(ngrams(Trump,3))),len(set(ngrams(Trump,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('thank', 'thank', 'thank'), 5), (('thank', 'thank', 'much'), 3), (('united', 'states', 'america'), 2), (('this', 'week', 'i'), 2), (('win', 'like', 'never'), 2), (('thank', 'much', 'thank'), 2), (('thank', 'much', 'everybody'), 1), (('much', 'everybody', 'wow'), 1), (('everybody', 'wow', 'thank'), 1), (('wow', 'thank', 'much'), 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter #Klasa do zliczania\n",
    "\n",
    "counts = Counter(list(ngrams(Trump,3)))\n",
    "print(counts.most_common(10)) #10 najpopularniejszych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> N-gramy dla znaków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_4grams = []\n",
    "\n",
    "for word in word_tokenize(ex_text):\n",
    "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('_', '_', '_', 'T'), ('_', '_', 'T', 'h'), ('_', 'T', 'h', 'i'), ('T', 'h', 'i', 's'), ('h', 'i', 's', '_'), ('i', 's', '_', '_'), ('s', '_', '_', '_')], [('_', '_', '_', 'i'), ('_', '_', 'i', 's'), ('_', 'i', 's', '_'), ('i', 's', '_', '_'), ('s', '_', '_', '_')], [('_', '_', '_', 'a'), ('_', '_', 'a', 'n'), ('_', 'a', 'n', '_'), ('a', 'n', '_', '_'), ('n', '_', '_', '_')], [('_', '_', '_', 'e'), ('_', '_', 'e', 'x'), ('_', 'e', 'x', 'a'), ('e', 'x', 'a', 'm'), ('x', 'a', 'm', 'p'), ('a', 'm', 'p', 'l'), ('m', 'p', 'l', 'e'), ('p', 'l', 'e', '_'), ('l', 'e', '_', '_'), ('e', '_', '_', '_')], [('_', '_', '_', 'p'), ('_', '_', 'p', 'a'), ('_', 'p', 'a', 'p'), ('p', 'a', 'p', 'e'), ('a', 'p', 'e', 'r'), ('p', 'e', 'r', '_'), ('e', 'r', '_', '_'), ('r', '_', '_', '_')]]\n"
     ]
    }
   ],
   "source": [
    "print(generated_4grams[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_', '_', '_', 'T'), ('_', '_', 'T', 'h'), ('_', 'T', 'h', 'i'), ('T', 'h', 'i', 's'), ('h', 'i', 's', '_'), ('i', 's', '_', '_'), ('s', '_', '_', '_'), ('_', '_', '_', 'i'), ('_', '_', 'i', 's'), ('_', 'i', 's', '_')]\n"
     ]
    }
   ],
   "source": [
    "generated_4grams = [word for sublist in generated_4grams for word in sublist] #żeby nie bylo listy list jak wyzej\n",
    "\n",
    "print(generated_4grams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['___T', '__Th', '_Thi', 'This', 'his_', 'is__', 's___', '___i', '__is', '_is_', 'is__', 's___', '___a', '__an', '_an_', 'an__', 'n___', '___e', '__ex', '_exa', 'exam', 'xamp', 'ampl', 'mple', 'ple_', 'le__', 'e___', '___p', '__pa', '_pap', 'pape', 'aper', 'per_', 'er__', 'r___', '___i', '__in', '_in_', 'in__', 'n___', '___w', '__wh', '_whi', 'whic', 'hich', 'ich_', 'ch__', 'h___', '___I', '__I_', '_I__', 'I___', '___s', '__su', '_sum', 'summ', 'umme', 'mmed', 'med_', 'ed__', 'd___', '___u', '__up', '_up_', 'up__', 'p___', '___s', '__so', '_som', 'some', 'ome_', 'me__', 'e___', '___b', '__ba', '_bas', 'basi', 'asic', 'sic_', 'ic__', 'c___', '___c', '__co', '_con', 'conc', 'once', 'ncep', 'cept', 'epts', 'pts_', 'ts__', 's___', '___r', '__re', '_ref', 'reff', 'effe', 'ffer', 'feri', 'erin', 'ring', 'ing_', 'ng__', 'g___', '___t', '__to', '_to_', 'to__', 'o___', '___v', '__vi', '_vir', 'viru', 'irus', 'ruse', 'uses', 'ses_', 'es__', 's___', '___.', '__._', '_.__', '.___']\n"
     ]
    }
   ],
   "source": [
    "ng_list_4grams = generated_4grams\n",
    "for idx, val in enumerate(generated_4grams):\n",
    "    ng_list_4grams[idx] = ''.join(val)\n",
    "    \n",
    "print(ng_list_4grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Przykład (podobieństwo zdań)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"The godfather is great\"\n",
    "s2 = \"The godfather is not great\"\n",
    "\n",
    "slowa = [\"godfather\", \"not\", \"great\"]\n",
    "t1 = [1, -1, 1]\n",
    "t2 = [1, 1, 1]\n",
    "\n",
    "bi_s = [\"godfather\", \"not\", \"great\", \"godfather great\", \"godfather not\", \"not great\"]\n",
    "t1b = [1, -1, 1, 1, -1, -1]\n",
    "t2b = [1, 1, 1, -1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333333333333337\n",
      "\n",
      "-0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "cos_sim = lambda a, b: np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "print(cos_sim(t1, t2))\n",
    "print()\n",
    "print(cos_sim(t1b, t2b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generowanie tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('gfr.txt','r')  \n",
    "text = file.read().replace('\\n','')\n",
    "GF = [x.lower() for x in word_tokenize(text) if x not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amerigo', 'bonasera', 'sat', 'in', 'new', 'york', 'criminal', 'court', 'number', '3', 'and', 'waited', 'for', 'justice', 'vengeance', 'on', 'the', 'men', 'who', 'had', 'so', 'cruelly', 'hurt', 'his', 'daughter', 'who', 'had', 'tried', 'to', 'dishonour', 'her', 'the', 'judge']\n"
     ]
    }
   ],
   "source": [
    "print(GF[:33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('amerigo', 'bonasera', 'sat'), ('bonasera', 'sat', 'in'), ('sat', 'in', 'new'), ('in', 'new', 'york'), ('new', 'york', 'criminal')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "N3_GM = list(ngrams(GF,3))\n",
    "print(N3_GM[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sonny', 'was', 'screwing'), ('sonny', 'was', 'chafing'), ('sonny', 'was', 'instantly'), ('sonny', 'was', 'hot'), ('sonny', 'was', 'sprawled'), ('sonny', 'was', 'smiling'), ('sonny', 'was', 'white-faced'), ('sonny', 'was', 'smart'), ('sonny', 'was', 'gayer'), ('sonny', 'was', 'scratching'), ('sonny', 'was', 'laughing'), ('sonny', 'was', 'laughing'), ('sonny', 'was', 'sixteen'), ('sonny', 'was', 'obviously'), ('sonny', 'was', 'formally'), ('sonny', 'was', 'a'), ('sonny', 'was', 'his'), ('sonny', 'was', 'not'), ('sonny', 'was', 'a'), ('sonny', 'was', 'a'), ('sonny', 'was', 'jubilant'), ('sonny', 'was', 'worried'), ('sonny', 'was', 'under'), ('sonny', 'was', 'sure'), ('sonny', 'was', 'capable'), ('sonny', 'was', 'not'), ('sonny', 'was', 'killed')]\n"
     ]
    }
   ],
   "source": [
    "x = \"sonny\"\n",
    "y = \"was\"\n",
    "print([w for w in N3_GM if w[0] == x and w[1] == y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vito was astonished but was still a little girls held his 169 hands one on they came to power the suftcring people learned never to hate sonny for his interview\n"
     ]
    }
   ],
   "source": [
    "x = \"vito\"\n",
    "y = \"was\"\n",
    "N = 30\n",
    "txt = [x,y]\n",
    "for i in range(N-2):\n",
    "    tg = [w for w in N3_GM if w[0] == txt[-2] and w[1] == txt[-1]]\n",
    "    random.shuffle(tg)\n",
    "    txt.append(tg[0][2])\n",
    "print( ' '.join(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Generowanie nowego tekstu z użyciem HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank \n",
    "\n",
    "train_set =  treebank.tagged_sents()[:3000]\n",
    "test_set =   treebank.tagged_sents()[3000:]\n",
    "\n",
    "from nltk.tag import hmm\n",
    "\n",
    "HMM_model = hmm.HiddenMarkovModelTrainer() \n",
    "HMM_tagger = HMM_model.train_supervised(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_backward_probability', '_best_path', '_best_path_simple', '_cache', '_check_params', '_confusion_cached', '_create_cache', '_exhaustive_entropy', '_exhaustive_point_entropy', '_forward_probability', '_output_logprob', '_outputs', '_outputs_vector', '_priors', '_sample_probdist', '_states', '_symbols', '_tag', '_train', '_transform', '_transitions', '_transitions_matrix', '_update_cache', 'accuracy', 'best_path', 'best_path_simple', 'confusion', 'entropy', 'evaluate', 'evaluate_per_tag', 'f_measure', 'log_probability', 'point_entropy', 'precision', 'probability', 'random_sample', 'recall', 'reset_cache', 'tag', 'tag_sents', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "print(dir(HMM_tagger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10779, 46)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ts = [item for sublist in train_set for item in sublist]\n",
    "\n",
    "len(set([x[0] for x in Ts])), len(set([x[1] for x in Ts])) #ile bylo wszystkich slow i tagow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method HiddenMarkovModelTagger._transitions_matrix of <HiddenMarkovModelTagger 46 states and 10779 output symbols>>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMM_tagger._transitions_matrix #macierz przejscia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35720553e+000, -2.81232644e+000, -6.06115900e+000, ...,\n",
       "         0.00000000e+000, -1.00000000e+300, -1.00000000e+300],\n",
       "       [-2.67482820e+000, -1.00000000e+300, -3.83289001e+000, ...,\n",
       "        -1.00000000e+300, -1.00000000e+300, -1.00000000e+300],\n",
       "       [-5.70162826e+000, -5.29882630e+000, -2.61679483e+000, ...,\n",
       "        -1.00000000e+300, -1.00000000e+300,  0.00000000e+000],\n",
       "       ...,\n",
       "       [-1.28309113e+001, -1.00000000e+300, -1.00000000e+300, ...,\n",
       "        -1.00000000e+300, -1.00000000e+300, -1.00000000e+300],\n",
       "       [-1.00000000e+300, -1.18837888e+001, -1.00000000e+300, ...,\n",
       "        -1.00000000e+300, -1.00000000e+300, -1.00000000e+300],\n",
       "       [-1.00000000e+300, -1.00000000e+300, -1.00000000e+300, ...,\n",
       "        -1.00000000e+300, -1.00000000e+300, -1.00000000e+300]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMM_tagger._transitions_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('out', 'IN'),\n",
       " ('2019', 'CD'),\n",
       " ('York', 'NNP'),\n",
       " ('Bush', 'NNP'),\n",
       " ('claims', 'VBZ'),\n",
       " ('unfair', 'JJ'),\n",
       " ('banks', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('opinion', 'NN')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tu właściwy kod\n",
    "HMM_tagger.random_sample(rng=random.Random(), length = 10) #generowanie przykladowej sekwencji o dlugosci 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Uzupełnienie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ja' 'lubię' 'nie' 'owoce' 'słodyczy' 'warzywa']\n",
      "(3, 6)\n",
      "\n",
      "[[1.         0.41095209 0.32600383]\n",
      " [0.41095209 1.         0.32600383]\n",
      " [0.32600383 0.32600383 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "corpus = [\n",
    "    'Ja lubię warzywa',\n",
    "    'Ja lubię owoce',\n",
    "    'Ja nie lubię słodyczy'\n",
    "]\n",
    "vectorizer = TfidfVectorizer() #mozna wrzucic do srodka max_features = ?? \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.shape)\n",
    "print()\n",
    "print(cosine_similarity(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Mini projekty (zadanie do wyboru na następne zajecia, prezentowane podczas zajęć)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie1: Zbuduj model generujący teskt o zadanej długości w oparciu o poznane techniki.\n",
    "  \n",
    "<br>\n",
    " \n",
    "Możesz uwzględnić (opcjonalnie)\n",
    "    \n",
    "- zgromadź możliwie dużą liczbę tekstów\n",
    "- spróbuj określić średnią (losową) długość zdania (powinno się kiedyś sensownie skończyć)\n",
    "- można oprzeć się o n-gramy (potestuj różne długości żeby wybrać optymalną długość, możesz uwzględnić wygładzanie - smoothing)\n",
    "- uwzględnij POS tagi (z użyciem HMM lub w inny sposób)\n",
    "- pamiętaj o odpowiednim przygotowaniu danych (w tym odpowiedniej zamianie liter na małe/duże)\n",
    "- można posiłkować się też synonimami słów żeby generowane teksty były bardziej różnorodne\n",
    "- ...\n",
    "    \n",
    "Finalnie, wygeneruj przykładowy tekst z ok 20 zdaniami.\n",
    "    \n",
    "     \n",
    "<br> Prezentacja (ok 3 min) powinna obejmować 3 slajdy oraz omówienie\n",
    "- jakie teksty wybrano, ile i dlaczego takie (1 slajd)\n",
    "- jakie techniki (+ich omówienie) zastosowano żeby generowane teksty jak najlepiej przypominały język naturalny (1 slajd+omówienie skryptu)\n",
    "- przykład wygenerowanego tekstu (1 slajd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie2: Zbuduj model, którego celem będzie weryfikacja czy dany tekst jest plagiatem innego tekstu. \n",
    "    \n",
    "<br>\n",
    "    \n",
    "Możesz uwzględnić (opcjonalnie)\n",
    "- podobieństwo tekstów możesz określić na podstawie ilości wspólnych n-gramów, potestuj rózne długości\n",
    "- podobieństwo cosinusowe na reprezentacjach TF-IDF\n",
    "- być może warto dokonać lemmatyzacji, stemmingu, ...\n",
    "- może warto spojrzeć na znaczenie słów, w tym detekcja synonimów (metryka Wu-Palmera)\n",
    "- końcowa ocena może (i powinna) opierać się na wielu miarach podobieństwa\n",
    "- zwróć uwagę, że jeden tekst może stanowić podzbiór innego tekstu (w takim przypadku podobieństwo powinno być również odpowiednio wysokie, np. miara Jaccarda)\n",
    "- ...\n",
    "    \n",
    "<br> Prezentacja (ok 3 min) powinna obejmować 3 slajdy oraz omówienie\n",
    "- w jaki sposób zdefiniowano podobieństwo między tekstami (1 slajd)\n",
    "- jakie techniki (+ich omówienie) zastosowano żeby określić to podobieństwo (1 slajd+omówienie skryptu)\n",
    "- przedstawienie kilku przykładów: tekst vs ten sam tekst, tekst vs fragmenty tego samego tekstu, tekst vs fragmenty tego samego tekstu gdzie wybrane słowa zostały zastąpione synonimami, tekst vs losowy tekst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie3: Zbuduj model, który będzie znajdywał w tekście najbardziej relewantą informację na zadane pytanie.\n",
    "    \n",
    "<br>\n",
    "\n",
    "Możesz uwzględnić (opcjonalnie)\n",
    "- podobieństwo tekstów możesz określić na podstawie ilości wspólnych n-gramów, potestuj rózne długości\n",
    "- podobieństwo cosinusowe na reprezentacjach TF-IDF\n",
    "- być może warto dokonać lemmatyzacji, stemmingu, ...\n",
    "- być może warto podzielić tekst na zdania (sent_tokenize)\n",
    "- może warto spojrzeć na znaczenie słów, w tym detekcja synonimów (metryka Wu-Palmera)\n",
    "- końcowa ocena może (i powinna) opierać się na wielu miarach podobieństwa\n",
    "- zwróć uwagę, że zdania mogą być róznej długości, a to może również determinować wybór finalnego fragmentu (a nie zawsze powinno)\n",
    "- ...\n",
    "    \n",
    "<br> Prezentacja (ok 3 min) powinna obejmować 3 slajdy oraz omówienie\n",
    "- w jaki sposób zdefiniowano podobieństwo między tekstami (1 slajd)\n",
    "- jakie techniki (+ich omówienie) zastosowano żeby określić to podobieństwo (1 slajd+omówienie skryptu)\n",
    "- przedstawienie kilku przykładów: tekst, pytanie -> otrzymany fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykładowe działanie na: \n",
    "\n",
    "The Godfather is a classic novel written by Mario Puzo in 1969 and later adapted into a legendary film trilogy directed by Francis Ford Coppola. The story revolves around the powerful Italian-American Mafia family, the Corleones, led by Don Vito Corleone. The novel and films explore themes of power, loyalty, family, and the moral complexities of crime. Don Vito Corleone is a respected and feared Mafia boss who believes in honor and justice within his own code of ethics. He helps those who come to him in need but expects loyalty and favors in return. His three sons—Sonny, Fredo, and Michael—each have different attitudes toward the family's criminal empire. The youngest of Don Vito’s sons is named Michael, and he initially wants nothing to do with the family business. However, after a series of tragic events, Michael is drawn into the world of organized crime and eventually becomes the new head of the family. The transformation of Michael Corleone is one of the most compelling aspects of The Godfather. At first, he is an idealistic war hero who wishes to lead an honest life, but circumstances force him to make ruthless decisions. Over time, he becomes even more powerful and feared than his father, sacrificing his morality and personal relationships for the sake of the family’s survival. The Godfather films, particularly the first two, are considered among the greatest movies of all time. They depict not only the brutal reality of the Mafia but also the deep emotional bonds and conflicts within the Corleone family. The saga remains a timeless masterpiece that continues to captivate audiences worldwide.\n",
    "\n",
    "What is the name of the youngest son of Vito?\n",
    "\n",
    "może zwrócić zdanie (lub jego część): The youngest of Don Vito’s sons is named Michael, and he initially wants nothing to do with the family business."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
