{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15bfc6dc",
   "metadata": {},
   "source": [
    "<h3> Rozważamy tweety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815ed306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_data():\n",
    "    return csv.reader(open(\"training.1600000.processed.noemoticon.csv\", \"rt\", encoding=\"latin-1\"))\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for i, line in enumerate(get_data()): \n",
    "    labels.append(int(int(line[0])/4))\n",
    "    texts.append(line[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8294238c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36ab057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eeb518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24d4395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08026124",
   "metadata": {},
   "source": [
    "<h2> Sieci rekurencyjne (LSTM), model do klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f30693",
   "metadata": {},
   "source": [
    "<h4> LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8499b6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam trening\n",
      "Epoch 1 | Loss: 36.1836\n",
      "Epoch 2 | Loss: 25.0502\n",
      "Epoch 3 | Loss: 17.1462\n",
      "Epoch 4 | Loss: 11.7116\n",
      "Epoch 5 | Loss: 8.2658\n",
      "Epoch 6 | Loss: 6.2544\n",
      "Epoch 7 | Loss: 4.6534\n",
      "Epoch 8 | Loss: 3.9906\n",
      "Epoch 9 | Loss: 3.0620\n",
      "Epoch 10 | Loss: 2.2881\n",
      "Epoch 11 | Loss: 2.4341\n",
      "Epoch 12 | Loss: 2.0429\n",
      "Epoch 13 | Loss: 1.4872\n",
      "Epoch 14 | Loss: 1.3573\n",
      "Epoch 15 | Loss: 1.3931\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\") #duze litery będa automatyczie na małą zamienione, mozna by uzyc word_tokenize tez\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "\n",
    "#tekst -> indeksy\n",
    "def text_to_indices(text):\n",
    "    return [glove.stoi.get(token, 0) for token in tokenizer(text)]  # 0 = indeks dla nieznanych słów\n",
    "\n",
    "\n",
    "#klasa dla danych\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.data = [torch.tensor(text_to_indices(t), dtype=torch.long) for t in texts]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "dataset = TextDataset(X_train, y_train)\n",
    "\n",
    "#collate_fn do obsługi paddingu i długości\n",
    "def collate_batch(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    #sortuje malejąco po długości (wymagane przez pack_padded_sequence)\n",
    "    seq_lens = [len(seq) for seq in sequences]\n",
    "    sorted_indices = sorted(range(len(seq_lens)), key=lambda i: -seq_lens[i])\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "\n",
    "    #paduje sekwencje do najdłuższej\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True)  \n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])  # rzeczywiste długości\n",
    "\n",
    "    return padded_seqs, lengths, labels\n",
    "\n",
    "#DataLoader z naszym collate_fn i do obsługi batcha\n",
    "loader = DataLoader(dataset, batch_size=250, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "#Wlasciwy model\n",
    "class Klasyfikator_LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=False)  # Wbudowane embeddingi GloVe, freee=F sprawia ze embeddingi nie będą aktualizowane\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x) \n",
    "        packed_input = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=True) #chowamy paddingi przed RNN\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input) #dane przechodzą przez LSTM\n",
    "        last_hidden = hn[-1]  #bierzemy ostatni stan ukryty\n",
    "        output = self.fc(last_hidden)  #klasyfikacja na podstawie ostatniego stanu ukrytego\n",
    "        return output\n",
    "\n",
    "embedding_dim = 50\n",
    "hidden_size = 20\n",
    "num_classes = 2\n",
    "\n",
    "model = Klasyfikator_LSTM(embedding_dim, hidden_size, num_classes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Rozpoczynam trening\")\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        inputs, lengths, targets = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, lengths)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ecad7",
   "metadata": {},
   "source": [
    "<h3> Jak to wygląda na zbiorze treningowym?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555ed32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Dokładność: 0.9957\n",
      "\n",
      "📊 Szczegółowy raport:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      7981\n",
      "           1       0.99      1.00      1.00      8019\n",
      "\n",
      "    accuracy                           1.00     16000\n",
      "   macro avg       1.00      1.00      1.00     16000\n",
      "weighted avg       1.00      1.00      1.00     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ewaluacja modelu\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        inputs, lengths, targets = batch\n",
    "        outputs = model(inputs, lengths)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(targets.tolist())\n",
    "\n",
    "#dokładnosć\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\n🔍 Dokładność: {acc:.4f}\")\n",
    "\n",
    "#szczegółowy raport\n",
    "print(\"\\n📊 Szczegółowy raport:\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738fe75d",
   "metadata": {},
   "source": [
    "<h3> Jak to wygląda na zbiorze testowym?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a75b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Dokładność: 0.7262\n",
      "\n",
      "📊 Szczegółowy raport:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73      2018\n",
      "           1       0.72      0.73      0.73      1982\n",
      "\n",
      "    accuracy                           0.73      4000\n",
      "   macro avg       0.73      0.73      0.73      4000\n",
      "weighted avg       0.73      0.73      0.73      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset2 = TextDataset(X_test, y_test)\n",
    "loader2 = DataLoader(dataset2, batch_size=250, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader2:\n",
    "        inputs, lengths, targets = batch\n",
    "        outputs = model(inputs, lengths)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(targets.tolist())\n",
    "\n",
    "#dokładnosć\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\n🔍 Dokładność: {acc:.4f}\")\n",
    "\n",
    "#szczegółowy raport\n",
    "print(\"\\n📊 Szczegółowy raport:\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c2f8d",
   "metadata": {},
   "source": [
    "<h3> Inne wersje sieci rekurencyjnych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f414e",
   "metadata": {},
   "source": [
    "* biLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ed02a",
   "metadata": {},
   "source": [
    "self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "self.fc = nn.Linear(hidden_size * 2, num_classes) #bo wyjscie ma teraz wymiar 2 * hidden_size\n",
    "\n",
    "--> W forward:\n",
    "\n",
    "packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "forward_final = hn[0]\n",
    "\n",
    "backward_final = hn[1] \n",
    "\n",
    "final_hidden = torch.cat((forward_final, backward_final), dim=1)\n",
    "\n",
    "output = self.fc(final_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a89422",
   "metadata": {},
   "source": [
    "* GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0abc72",
   "metadata": {},
   "source": [
    "self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)  \n",
    "self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "--> w forward\n",
    "\n",
    "packed_output, hn = self.gru(packed_input) \n",
    "\n",
    "last_hidden = hn[-1] \n",
    "\n",
    "output = self.fc(last_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1aa8d9",
   "metadata": {},
   "source": [
    "<h1> SNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ebec5",
   "metadata": {},
   "source": [
    " https://nlp.stanford.edu/projects/snli/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae1864",
   "metadata": {},
   "source": [
    "<h4> Zadanie1 (2 pkt). Pobierz dane. Zadanie polega na klasyfikacji pary zdań (tzw. przesłanka i hipoteza) do jednej z 3 kategorii przy pomocy sieci rekurencyjnych (LSTM/biLSTM lub GRU):\n",
    "    \n",
    "   <br> \n",
    "    \n",
    "- E (entailment, czyli z pierwszego zdania wynika drugie)\n",
    "- C (contradiction, czyli drugie zdanie jest w sprzeczności z pierwszym)\n",
    "- N (neutral, drugie zdanie jest neutralne w stosunku do pierwszego)\n",
    "    \n",
    "    <br>\n",
    "   \n",
    "Dla przykładu:\n",
    "    \n",
    "(Dzisiaj jest czwartek. Jutro jest piątek) -> E\n",
    "    \n",
    "(Dzisiaj jest czwartek. Jutro jest poniedziałek) -> C\n",
    "    \n",
    "(Dzisiaj jest czwartek. Wczoraj padało) -> N\n",
    "    \n",
    "    \n",
    " Uwagi i wskazówki:\n",
    "- zastosuj embeddingi 50D (lub większe jeżeli będzie taka konieczność)\n",
    "- w wyniku kodowania  zdań wejściowych otrzymujemy dwa wektory (wektorów) dla przesłanki i hipotezy. Te wektory najlepiej następnie skonktatenować, ewentualnie z jakimś przerywnikiem typu \"implies\"\n",
    "- używaj batchów (np. 1000)\n",
    "- wyróżnij zbiór walidacyjny celem określenia optymalnej topologii sieci\n",
    "- analizuj dokładność na zbiorze treningowym i walidacyjnym co epokę (także graficznie)\n",
    "- przedstaw graficznie wartość funkcji kosztu w kolejnych epokach\n",
    "- potestuj różne zestawy parametrów (jak hidden size, funkcje aktywacji, dropout, batchnormalizację itd...)\n",
    "- sprawdź dokładność na zbiorze testowym (przynajmniej 10% wszystkich danych) dla najlepszego modelu\n",
    "- jeżeli proces trenowania będzie trwał zbyt długo to zmniejsz rozważany zbiór danych! :) Mimo wszystko finalnie powinieneś użyć pełnego zbioru (zapraszam do pracowni! :))\n",
    "    \n",
    "    \n",
    "Dokładność na zbiorze testowym vs liczba punktów za zadanie   \n",
    "* poniżej 65 --> 0.5 pkt\n",
    "* 65-70 --> 1  pkt\n",
    "* 70-75 --> 1.5 pkt\n",
    "* ponad 75 --> 2 pkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a0a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  annotator_labels         captionID     gold_label               pairID  \\\n",
      "0        [neutral]  3416050480.jpg#4        neutral  3416050480.jpg#4r1n   \n",
      "1  [contradiction]  3416050480.jpg#4  contradiction  3416050480.jpg#4r1c   \n",
      "2     [entailment]  3416050480.jpg#4     entailment  3416050480.jpg#4r1e   \n",
      "3        [neutral]  2267923837.jpg#2        neutral  2267923837.jpg#2r1n   \n",
      "4     [entailment]  2267923837.jpg#2     entailment  2267923837.jpg#2r1e   \n",
      "\n",
      "                                           sentence1  \\\n",
      "0  A person on a horse jumps over a broken down a...   \n",
      "1  A person on a horse jumps over a broken down a...   \n",
      "2  A person on a horse jumps over a broken down a...   \n",
      "3              Children smiling and waving at camera   \n",
      "4              Children smiling and waving at camera   \n",
      "\n",
      "                              sentence1_binary_parse  \\\n",
      "0  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
      "1  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
      "2  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
      "3  ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
      "4  ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
      "\n",
      "                                     sentence1_parse  \\\n",
      "0  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
      "1  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
      "2  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
      "3  (ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...   \n",
      "4  (ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...   \n",
      "\n",
      "                                           sentence2  \\\n",
      "0  A person is training his horse for a competition.   \n",
      "1      A person is at a diner, ordering an omelette.   \n",
      "2                  A person is outdoors, on a horse.   \n",
      "3                  They are smiling at their parents   \n",
      "4                         There are children present   \n",
      "\n",
      "                              sentence2_binary_parse  \\\n",
      "0  ( ( A person ) ( ( is ( ( training ( his horse...   \n",
      "1  ( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...   \n",
      "2  ( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...   \n",
      "3  ( They ( are ( smiling ( at ( their parents ) ...   \n",
      "4             ( There ( ( are children ) present ) )   \n",
      "\n",
      "                                     sentence2_parse  \n",
      "0  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...  \n",
      "1  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...  \n",
      "2  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...  \n",
      "3  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...  \n",
      "4  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"snli_1.0/snli_1.0_train.jsonl\", lines = True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3f8e9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550152, 10)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40d305eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A person on a horse jumps over a broken down airplane.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentence1\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d216ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A person is training his horse for a competition.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentence2\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a9652a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"gold_label\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07458b92",
   "metadata": {},
   "source": [
    "<h2> Dostrajanie modelu (Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4d6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e94b0",
   "metadata": {},
   "source": [
    "BERT - oparty o tzw. architekturę Transformers, jak trenowany?\n",
    "\n",
    "Zadania:\n",
    "- Masked Language Modeling (MLM) — zgadywanie brakujących słów w zdaniach,\n",
    "- Next Sentence Prediction (NSP) — przewidywanie, czy zdanie B jest kontynuacją zdania A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9a446",
   "metadata": {},
   "source": [
    "Przykład"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1fc77",
   "metadata": {},
   "source": [
    "* bert-base-uncased - ok 110 milionów parametrów (w tym embeddingi tokenów, parametry każdej warstwy Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42efee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #tokenizator na modelu językowym BERT (ang) -> dobre rozumienie jezyka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e0356",
   "metadata": {},
   "source": [
    "Inne:\n",
    "* bert-base-multilingual-cased <- wielojęzykowy\n",
    "* polbert <- typowo polski, trenowany m.in. na Wikipedia\n",
    "* herbert-base-case <- też polski (https://huggingface.co/allegro/herbert-base-cased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420417f3",
   "metadata": {},
   "source": [
    "* Dodaje warstwę do klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c59314e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) #buduje model klasyfikacji w oparciu o BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffbc4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"This product is amazing! Definitely worth buying, it meets all my expectations.\",\n",
    "    \"Customer service was really helpful and fast, I will definitely come back again!\",\n",
    "    \"The quality of this product is top-notch, I am very satisfied with the purchase.\",\n",
    "    \"Everything works flawlessly, I am really happy with this transaction!\",\n",
    "    \"I highly recommend this store, delivery was fast, and the product was exactly as described.\",\n",
    "    \n",
    "    \"Unfortunately, the product was damaged upon arrival, and customer service did not respond to my inquiry.\",\n",
    "    \"This product completely failed to meet my expectations. I'm very disappointed with the quality.\",\n",
    "    \"Delivery was delayed by several days, and getting in touch with the company was very difficult. I do not recommend.\",\n",
    "    \"The product doesn't work as described, and the company offers no technical support.\",\n",
    "    \"The website was confusing and poorly designed, making the whole buying process frustrating.\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c44d97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(reviews, truncation=True, padding=True) #przycina dluzsze teksty (modele czesto mają limit); padding dodaje 0 zeby wyrownac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f249975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = TextDataset(encodings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1abe9a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.263900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.48978143334388735, metrics={'train_runtime': 15.6797, 'train_samples_per_second': 1.913, 'train_steps_per_second': 0.957, 'total_flos': 385416585000.0, 'train_loss': 0.48978143334388735, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", #gdzie zapisze model\n",
    "    num_train_epochs=3,  #liczba epok\n",
    "    per_device_train_batch_size=2, #rozmiar batcha\n",
    "    logging_dir=\"./logs\", #gdzie zapisze logi (dane z treningu)\n",
    "    logging_steps=2,  #co ile batchow zapisac logi ->> mozna na bieżaco sprawdzac jak model sie uczy\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset, #mozna tez dorzucic zbior walidacyjny eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83f4d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Very nice product.\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=1)\n",
    "print(predictions)  #0 lub 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eeac82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Very bad product.\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=1)\n",
    "print(predictions)  #0 lub 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0168775",
   "metadata": {},
   "source": [
    "<h4> Zadanie2 (2 pkt): Wybierz co najmniej 3 kandydatów na prezydenta. Stwórz zbiór co najmniej 100 wypowiedzi na osobę, a następnie zbuduj klasyfikator na bazie istniejącego modelu - w oparciu o wypowiedź tekst przewiduje który kandydat mógł to powiedzieć.\n",
    "    \n",
    "    \n",
    "Uwagi i wskazówki:\n",
    "- możesz oprzeć się na modelu herbert-base-cased \n",
    "- pobieranie wypowiedzi najlepiej zrealizuj w sposób automatyczny (np. selenium)\n",
    "- może warto wybrać wypowiedzi z debat, poszukaj odpowiednich bibliotek w python do pobierania napisów z youtube; w przypadku dłuższych wypowiedzi podziel je na krótsze -> kilka zdań\n",
    "- zadbaj o jakość danych! Pamiętaj, że to jest podstawa sukcesu! (dobre dane + dobry model = sukces, słabe dane + dowolony model = porażka). Usuń ewentualne duplikaty. W przypadku tweetów rozważ czy warto uwzględnić także przekierowania\n",
    "- podziel dane na zbiór trenignowy/walidacyjny/testowy (7:2:1); wyznacz dokładności na tych zbiorach\n",
    "- sprawdz bezpośrednio model na przykładowych wypowiedziach \n",
    "- może warto rozbudować część klasyfikacyjną, poszukaj jak to zrobić"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c40dee5",
   "metadata": {},
   "source": [
    "<h4> Uzupełnienie (automatyczne pobieranie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0cd73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad52aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(\"C:\\\\Users\\\\Adrian\\\\Desktop\\\\Dydaktyka\\\\NLP\\\\web\\\\msedgedriver.exe\")\n",
    "browser = webdriver.Edge(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e7e7922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Join me tomorrow, April 29th—in Warren, Michigan at 6:00PM Eastern!\n",
      "A conversation about online child predators with Homeland Security and John Rich: PLS Repost!\n",
      "Sleepy Joe Biden, THE WORST PRESIDENT IN THE HISTORY OF THE UNITED STATES, has allowed millions and millions of Criminals, many of them murderers, drug dealers, and people released from prisons and mental institutions from all around the world, to enter our Country through it’s\n",
      "Following my Day One Executive Order, the Office of Personnel Management will be issuing new Civil Service Regulations for career government employees. Moving forward, career government employees, working on policy matters, will be classified as “Schedule Policy/Career,” and will\n",
      "This is the hand of the man that the Democrats feel should be brought back to the United States, because he is such “a fine and innocent person.” They said he is not a member of MS-13, even though he’s got MS-13 tattooed onto his knuckles, and two Highly Respected Courts found\n",
      "The United States has a chance to do something that should have been done DECADES AGO. Don’t be Weak! Don’t be Stupid! Don’t be a PANICAN (A new party based on Weak and Stupid people!). Be Strong, Courageous, and Patient, and GREATNESS will be the result!\n"
     ]
    }
   ],
   "source": [
    "# Zmienna do przechowywania tweetów\n",
    "tweets = []\n",
    "tweets_to_scrape = 5 #ile chcemy \n",
    "\n",
    "url = \"https://twitter.com/realdonaldtrump\"\n",
    "browser.get(url)\n",
    "\n",
    "\n",
    "scroll_pause_time = 2  # czas na załadowanie nowych tweetów (przy przewijaniu)\n",
    "\n",
    "while len(tweets) < tweets_to_scrape:\n",
    "    tweet_elements = browser.find_elements(By.XPATH, '//article//div[@lang]')\n",
    "    for tweet_element in tweet_elements:\n",
    "        try:\n",
    "            tweet_text = tweet_element.text\n",
    "            print(tweet_text) #tego nie trzeba wyswietlac, wrzucilem tylko zeby pokazac\n",
    "            tweets.append(tweet_text.replace(\"\\n\",\"\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Cos poszlo nie tak ;/ {e}\")\n",
    "\n",
    "    if len(tweets) >= tweets_to_scrape:\n",
    "        break\n",
    "\n",
    "    #przewijanie w dół, aby załadować więcej tweetów\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977fe97",
   "metadata": {},
   "source": [
    "<h4> Uzupełnienie (regularyzacja)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb901e6a",
   "metadata": {},
   "source": [
    "W sieciach neuronowych często może dochodzić do zjawiska przetrenowania. Związane jest to m.in. z dużą swobodą w wyborze liczby parametrów. Jak temu przeciwdziałać?\n",
    "\n",
    "- użyć więcej danych do trenowania\n",
    "- zbudować mniejszą sieć (=mniej parametrów)\n",
    "- dzielenie wag (weight sharing), jak np w konwolucyjnych sieciach neuronowych zastosowanie kernela dla wszystkich pozycji w obrazie\n",
    "- odpowiednie wczesne zakończenie procesu uczenia (kontrolowane np poprzez zbiór walidacyjny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9b2f3",
   "metadata": {},
   "source": [
    "Co można zrobić więcej?\n",
    "- zwiększenie danych (często na podstawie istniejących przykładów)\n",
    "- normalizacja danych (np BatchNormalizacja)\n",
    "- uśrednianie modeli\n",
    "- dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650b496",
   "metadata": {},
   "source": [
    "-> Regularyzacja L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b66b20",
   "metadata": {},
   "source": [
    "- zapobiega przeuczeniu, zniechęca model do dużych wartości wag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01117357",
   "metadata": {},
   "source": [
    "-> Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05994e8",
   "metadata": {},
   "source": [
    "- losowo wyłączamy pewną grupę neuronów podczas uczenia (w różnych iteracjach inne), oznaczmy ten procent przez $r$ \n",
    "- powoduje uniezależnianie wag od innych,\n",
    "podczas testowania korzystamy z pełni połączonej sieci (która może być traktowana jako uśrednienie tych z dropoutem), konieczna jest modyfikacja wyjścia $z$ na $z(1-r)$,\n",
    "- efekt: uniezależnienie działania sieci od pojedynczych neuronów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bez\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features, h1, h2, n_classes):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_input_features, h1)\n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.linear3 = nn.Linear(h2, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l1 = self.linear1(x)\n",
    "        o1 = F.relu(l1)\n",
    "        l2 = self.linear2(o1)\n",
    "        o2 = F.relu(l2)\n",
    "        l3 = self.linear3(o2)\n",
    "        return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features, h1, h2, n_classes):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_input_features, h1)\n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.linear3 = nn.Linear(h2, n_classes)\n",
    "        self.d1 = nn.Dropout(0.2) \n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "    def forward(self, x):\n",
    "        o1 = F.relu(self.linear1(x))\n",
    "        o1 = self.d1(o1)\n",
    "        o2 = F.relu(self.linear2(o1))\n",
    "        o2 = self.d2(o2)\n",
    "        o3 = self.linear3(o2)\n",
    "    return o3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27975a5",
   "metadata": {},
   "source": [
    "-> Normalizacja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b8425",
   "metadata": {},
   "source": [
    "- Cel: ustabilizowanie i przyspieszenie procesu uczenia przez normalizację w każdym \"batchu\", stosujemy najczęściej przed funkcją aktywacji, stabilizuje zakres wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05237738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features, h1, h2, n_classes):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_input_features, h1)\n",
    "        self.bn1 = nn.BatchNorm1d(h1)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.bn2 = nn.BatchNorm1d(h2)\n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear3 = nn.Linear(h2, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o1 = self.linear1(x)\n",
    "        o1 = self.bn1(o1)\n",
    "        o1 = F.relu(o1)\n",
    "        o1 = self.d1(o1)\n",
    "\n",
    "        o2 = self.linear2(o1)\n",
    "        o2 = self.bn2(o2)\n",
    "        o2 = F.relu(o2)\n",
    "        o2 = self.d2(o2)\n",
    "\n",
    "        o3 = self.linear3(o2)\n",
    "        return o3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
