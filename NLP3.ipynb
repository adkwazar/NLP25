{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Części mowy (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Very Large Telescope (VLT) of the European Southern Observatory (ESO), an array of four individual telescopes in the Atacama desert, has given us a huge amount of new data about the universe. Researchers have now used it to find a group of six galaxies around a supermassive black hole, from when the Universe was just 0.9 billion years old - it's estimated to be 13.8 billion years old now. Black holes are thought to sit at the center of galaxies including the Milky Way. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the very large telescope (vlt) of the european southern observatory (eso), an array of four individual telescopes in the atacama desert, has given us a huge amount of new data about the universe.', \"researchers have now used it to find a group of six galaxies around a supermassive black hole, from when the universe was just 0.9 billion years old - it's estimated to be 13.8 billion years old now.\", 'black holes are thought to sit at the center of galaxies including the milky way.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text.lower()) #podzial na sentencje \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'very', 'large', 'telescope', '(', 'vlt', ')', 'of', 'the', 'european', 'southern', 'observatory', '(', 'eso', ')', ',', 'an', 'array', 'of', 'four', 'individual', 'telescopes', 'in', 'the', 'atacama', 'desert', ',', 'has', 'given', 'us', 'a', 'huge', 'amount', 'of', 'new', 'data', 'about', 'the', 'universe', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_sen1 = word_tokenize(sentences[0]) #podzial na slowa pierwszego zdania\n",
    "print(tokens_sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 4, 'of': 3, '(': 2, ')': 2, ',': 2, 'very': 1, 'large': 1, 'telescope': 1, 'vlt': 1, 'european': 1, ...})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokens_sen1)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('of', 3)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(2) #dwa najczęstsze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('very', 'RB'), ('large', 'JJ'), ('telescope', 'NN'), ('(', '('), ('vlt', 'NN'), (')', ')'), ('of', 'IN'), ('the', 'DT'), ('european', 'JJ'), ('southern', 'JJ'), ('observatory', 'NN'), ('(', '('), ('eso', 'NN'), (')', ')'), (',', ','), ('an', 'DT'), ('array', 'NN'), ('of', 'IN'), ('four', 'CD'), ('individual', 'JJ'), ('telescopes', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('atacama', 'NN'), ('desert', 'NN'), (',', ','), ('has', 'VBZ'), ('given', 'VBN'), ('us', 'PRP'), ('a', 'DT'), ('huge', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('data', 'NNS'), ('about', 'IN'), ('the', 'DT'), ('universe', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag #Części mowy\n",
    "\n",
    "tags = pos_tag(tokens_sen1)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Słowa nieistniejące "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gutvidual', 'JJ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags2 = pos_tag(['gutvidual']) #a jednak przewiduje... :)\n",
    "tags2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Słowa w zależności od kontekstu mogą mieć różne POS tagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('can', 'MD'), ('not', 'RB'), ('bear', 'VB'), ('this', 'DT'), ('headache', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(word_tokenize(\"I cannot bear this headache\"))) #bear jest czasownikiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Yesterday', 'NN'), (',', ','), ('I', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('bear', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(word_tokenize(\"Yesterday, I saw a bear\")))  #bear jest rzeczownikiem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Uczenie HMM - jaki POS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem:\n",
    "\n",
    "- dana jest sekwencja wyrazów  $x_1x_2x_3...x_n$  (przykładowo [\"I\", \"like\", \"dogs\"])\n",
    "- cel: znaleźć najbardziej prawdopodobną sekwencje tagów  $y_1y_2y_3...y_n$  (przykładowo [\"PRP\",\"VBP\", \"NNS\"]), czyli:\n",
    "\n",
    "$argmax_{y_1,y_2,y_3...,y_n}P(x_1,x_2,x_3,...,x_n,y_1,y_2,y_3,...,y_n)$\n",
    "\n",
    "- Jak? \n",
    "\n",
    "Algorytm Viterbiego\n",
    "\n",
    "- Stany:\n",
    "\n",
    "obserwacje = słowa\n",
    "\n",
    "stany ukryte = tagi POS\n",
    "\n",
    "\n",
    "- Uczenie algorytmu polega na wyznaczeniu:\n",
    "\n",
    "prawdopodobieństw przejść = prawdopodobieństwa typu P(VP|NP)\n",
    "\n",
    "prawdopodobieńsw emisji = prawdopodobieństwa typu P(John|VP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ PENN TREEBANK SAMPLE ]\\r http://www.cis.upenn.edu/~treebank/home.html\\r \\r This is a ~5% fragment of Penn Treebank, (C) LDC 1995.  It is made\\r available under fair use for the purposes of illustrating NLTK tools\\r for tokenizing, tagging, chunking and parsing.  '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank  #korpus do testowania algorytmow przewidujących częsci mowy\n",
    "\n",
    "treebank.readme().replace('\\n', ' ')[:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print(treebank.tagged_sents()[:2]) #pierwsze dwie otagowane sentencje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3914"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treebank.tagged_sents()) #ile wszystkich otagowanych sentencji?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import hmm  #Ukryte modele Markowa\n",
    "\n",
    "trainer = hmm.HiddenMarkovModelTrainer() #buduje model HMMM\n",
    "tagger = trainer.train_supervised(treebank.tagged_sents()) #trenuje model (metoda największej wiarygodności) na wszystkich otagowanych sentencjach z treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('such', 'PDT'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('nice', 'JJ'),\n",
       " ('day', 'NN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(['today','is','such','a','very','nice','day']) #patrze jak model sprawdza sie na przykladowych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('such', 'PDT'),\n",
       " ('a', 'DT'),\n",
       " ('picturesque', 'NNP'),\n",
       " ('day', 'NNP')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(['today','is','such','a','picturesque','day']) #patrze jak model sprawdza sie na innych przykladowych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('such', 'JJ'),\n",
       " ('a', 'DT'),\n",
       " ('picturesque', 'JJ'),\n",
       " ('day', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['today','is','such','a','picturesque','day']) #POS tagging przy uzyciu metody pos_tag, dla porownania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9815546902936152"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.accuracy(treebank.tagged_sents())  #jaka jest przewidywalnosc modelu na całym korpusie treebank?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie1: Podziel wyjściowy zbiór sentencji $treebank.tagged\\_sents()$ na dwa zbiory: pierwsze 3000 sentencje zapisz pod zmienną  $trained$, natomiast pozostałe sentencje zapisz pod zmienną $tested$. Zbuduj model tylko w oparciu o $trained$. Następnie wykonaj ewaluacje modelu zarówno na  $trained$  jak i  $tested$. Zastanów się z czego mogą wynikać rozbieżności."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Porcjowanie (Chunking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('very', 'RB'), ('large', 'JJ'), ('telescope', 'NN'), ('(', '('), ('vlt', 'NN'), (')', ')'), ('of', 'IN'), ('the', 'DT'), ('european', 'JJ'), ('southern', 'JJ'), ('observatory', 'NN'), ('(', '('), ('eso', 'NN'), (')', ')'), (',', ','), ('an', 'DT'), ('array', 'NN'), ('of', 'IN'), ('four', 'CD'), ('individual', 'JJ'), ('telescopes', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('atacama', 'NN'), ('desert', 'NN'), (',', ','), ('has', 'VBZ'), ('given', 'VBN'), ('us', 'PRP'), ('a', 'DT'), ('huge', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('data', 'NNS'), ('about', 'IN'), ('the', 'DT'), ('universe', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "grammar = \"chunk: {<DT>?<JJ>*<NN>}\" #DT - determiner/określnik, JJ - adjective/przymiotnik, NN - noun/rzeczonik\n",
    "chunker = RegexpParser(grammar) \n",
    "result = chunker.parse(tags) #tags zdefiniowane bylo wyzej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  the/DT\n",
      "  very/RB\n",
      "  (chunk large/JJ telescope/NN)\n",
      "  (/(\n",
      "  (chunk vlt/NN)\n",
      "  )/)\n",
      "  of/IN\n",
      "  (chunk the/DT european/JJ southern/JJ observatory/NN)\n",
      "  (/(\n",
      "  (chunk eso/NN)\n",
      "  )/)\n",
      "  ,/,\n",
      "  (chunk an/DT array/NN)\n",
      "  of/IN\n",
      "  four/CD\n",
      "  individual/JJ\n",
      "  telescopes/NNS\n",
      "  in/IN\n",
      "  (chunk the/DT atacama/NN)\n",
      "  (chunk desert/NN)\n",
      "  ,/,\n",
      "  has/VBZ\n",
      "  given/VBN\n",
      "  us/PRP\n",
      "  (chunk a/DT huge/JJ amount/NN)\n",
      "  of/IN\n",
      "  new/JJ\n",
      "  data/NNS\n",
      "  about/IN\n",
      "  (chunk the/DT universe/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'DT')\n"
     ]
    }
   ],
   "source": [
    "print(result[0]) #mozna sie odwolywac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie2: Zapisz do listy wszystkie chunki występujące w tekście a spełniajacy zadany (przez siebie) warunek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw() #rysowanie tego co wyzej (pojawia sie nowe okno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie3: Zinterpretuj poniższy chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"chunk: {<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\" #co oznacza wyrazenie regularne zapisane w {}?\n",
    "chunker = RegexpParser(grammar) \n",
    "result = chunker.parse(tags) #tags zdefiniowane bylo wyzej\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie4: Pobierz dowolny tekst (przez f = open(...)). Następnie wyznacz dla niego zaproponowany przez siebie chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Rozpoznawanie bytów (Entity Recognition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Albert', 'NNP'), ('Einstein', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('at', 'IN'), ('Ulm', 'NNP'), (',', ','), ('in', 'IN'), ('Württemberg', 'NNP'), (',', ','), ('Germany', 'NNP'), (',', ','), ('on', 'IN'), ('March', 'NNP'), ('14', 'CD'), (',', ','), ('1879', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens_sen2 = word_tokenize(\"Albert Einstein was born at Ulm, in Württemberg, Germany, on March 14, 1879.\")\n",
    "tags2 = pos_tag(tokens_sen2)\n",
    "print(tags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NE Albert/NNP Einstein/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  at/IN\n",
      "  (NE Ulm/NNP)\n",
      "  ,/,\n",
      "  in/IN\n",
      "  (NE Württemberg/NNP)\n",
      "  ,/,\n",
      "  (NE Germany/NNP)\n",
      "  ,/,\n",
      "  on/IN\n",
      "  March/NNP\n",
      "  14/CD\n",
      "  ,/,\n",
      "  1879/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "namedEnt = ne_chunk(tags2, binary = True) #przeszukiwanie pod kątem osób, miejsc itd...\n",
    "print(namedEnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie5: Znajdz wszystkie byty (entity) w tekście poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Google, LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Five technology companies in the U.S. information technology industry, alongside Amazon, Facebook, Apple, and Microsoft.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stemowanie (Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer() #Tworze stemer\n",
    "\n",
    "words = [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "\n",
    "for elem in words:\n",
    "    print(ps.stem(elem)) #stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cell', 'is', 'the', 'basic', 'structur', 'and', 'function', 'unit', 'of', 'all', 'form', 'of', 'life', '.']\n"
     ]
    }
   ],
   "source": [
    "#stemowanie zdan\n",
    "print([ps.stem(elem) for elem in word_tokenize(\"The cell is the basic structural and functional unit of all forms of life.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lematyzacja (Lemmatization) - \"lepszy Stemming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() #tworze Lemmatyzator \n",
    "print(lemmatizer.lemmatize(\"cats\")) #wywołuje metode lemmatize z tego Lemmatyzatora na słowie 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"feet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos = 'r')) #n-noun/rzeczownik (domyslnie), a-adjective/przymiotnik, v-verb/czasownik, r-adverb/przyslowek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Sieć słów (Wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'), Synset('automobile.v.01')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"automobile\") #jakie są synstety (zbiory synonimow) dla slowa automobile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names() #jakie są słowa w synsecie car.n.01; to synset obejmujący różne synonimy (lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition() #jaka jest definicja tego synsetu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples() #przykladowe uzycie w zdaniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"car\") #jakie są synsety slowa car?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "['car', 'railcar', 'railway_car', 'railroad_car']\n",
      "['car', 'gondola']\n",
      "['car', 'elevator_car']\n",
      "['cable_car', 'car']\n"
     ]
    }
   ],
   "source": [
    "#jak wyglada te 5 synsetow?\n",
    "\n",
    "for synset in wn.synsets(\"car\"):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hiponimy i hiperonimy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar = wn.synset(\"car.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('jeep.n.01')\n"
     ]
    }
   ],
   "source": [
    "types_of_cars = motorcar.hyponyms() #hiponim to takie slowo ktorego znaczenie semanatyczne pochodzi od innego slowa (hiperonimu) np rozowy to kolor, gęś to ptak\n",
    "print(types_of_cars[14]) #przykład hiponimu (synsetu zawierającego hiponimy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jeep', 'landrover']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('jeep.n.01').lemma_names() #slowa w synsecie jeep.n.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ambulance', 'beach_wagon', 'station_wagon', 'wagon', 'estate_car', 'beach_waggon', 'station_waggon', 'waggon', 'bus', 'jalopy', 'heap', 'cab', 'hack', 'taxi', 'taxicab', 'compact', 'compact_car', 'convertible', 'coupe', 'cruiser', 'police_cruiser', 'patrol_car', 'police_car', 'prowl_car', 'squad_car', 'electric', 'electric_automobile', 'electric_car', 'gas_guzzler', 'hardtop', 'hatchback', 'horseless_carriage', 'hot_rod', 'hot-rod', 'jeep', 'landrover', 'limousine', 'limo', 'loaner', 'minicar', 'minivan', 'Model_T', 'pace_car', 'racer', 'race_car', 'racing_car', 'roadster', 'runabout', 'two-seater', 'sedan', 'saloon', 'sport_utility', 'sport_utility_vehicle', 'S.U.V.', 'SUV', 'sports_car', 'sport_car', 'Stanley_Steamer', 'stock_car', 'subcompact', 'subcompact_car', 'touring_car', 'phaeton', 'tourer', 'used-car', 'secondhand_car']\n"
     ]
    }
   ],
   "source": [
    "#hiponimy dla slowa car (z synsetu car.n.01)\n",
    "print([lemma.name() for synset in types_of_cars for lemma in synset.lemmas()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('motor_vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "vehicles = motorcar.hypernyms() #hiperonimy (synsety) dla car.n.01\n",
    "print(vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['automotive_vehicle', 'motor_vehicle']\n"
     ]
    }
   ],
   "source": [
    "print(sorted([lemma.name() for synset in vehicles for lemma in synset.lemmas()])) #wszystkie hiperonimy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie6: Znajdź wszystkie hiponimy i hiperonimy dla słowa $dog$. Zacznij od wyszukania synsetów dla tego słowa, wybierz pierwszy z nich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Synonimy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['girl', 'miss', 'missy', 'young_lady', 'young_woman', 'fille', 'female_child', 'girl', 'little_girl', 'daughter', 'girl', 'girlfriend', 'girl', 'lady_friend', 'girl']\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "for syn in wn.synsets('girl'):\n",
    "    for lemma in syn.lemmas(): \n",
    "        synonyms.append(lemma.name())\n",
    "\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Antonimy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male_child', 'boy', 'son', 'boy']\n"
     ]
    }
   ],
   "source": [
    "antonyms = []\n",
    "for syn in wn.synsets(\"girl\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie7: Znajdź synonimy i antonimy słowa $happy$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Metryka Wu-Palmera - podobieństwo synsetów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bicycle.n.01')\n",
      "Synset('male_child.n.01')\n",
      "Synset('homo.n.02')\n"
     ]
    }
   ],
   "source": [
    "bike = wn.synsets('bicycle')[0]\n",
    "boy = wn.synsets('boy')[0]\n",
    "human = wn.synsets('human')[0]\n",
    "\n",
    "print(bike)\n",
    "print(boy)\n",
    "print(human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homo', 'man', 'human_being', 'human']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('homo.n.02').lemma_names() #zeby srpawdzic co tam jest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34782608695652173, 0.5217391304347826, 0.4444444444444444)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike.wup_similarity(human), boy.wup_similarity(human), boy.wup_similarity(bike) #licze na ile slowa (a formalnie synsety są podobne do siebie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie8: Porównaj podobieństwo słów dog, cat i fish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Uzupełnienie: Gramatyki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://coling.epfl.ch/TP/corr/TP-parsing-sol.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b'], ['a'], ['b', 'a']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.grammar import CFG\n",
    "from nltk.parse.generate import generate\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> A W | B S\n",
    "W -> A S | B W |\n",
    "A -> 'a'\n",
    "B -> 'b'\n",
    "\"\"\")\n",
    "\n",
    "list(generate(grammar, depth = 4)) #depth = maksymalna glębokosc drzewa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'a', 'a'],\n",
       " ['a', 'b', 'b'],\n",
       " ['a', 'b'],\n",
       " ['a'],\n",
       " ['b', 'a', 'b'],\n",
       " ['b', 'a'],\n",
       " ['b', 'b', 'a']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(generate(grammar, depth = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Pytanie: Jak powstało \"aaa\"? Z kolei na liście wyżej brak \"aba\", dlaczego?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Uzupełnienie: przetwarzanie języka polskiego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/models/pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #biblioteka do przetwarzania języka naturalnego, cos jak nltk\n",
    "nlp = spacy.load(\"pl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'aby',\n",
       " 'ach',\n",
       " 'acz',\n",
       " 'aczkolwiek',\n",
       " 'aj',\n",
       " 'albo',\n",
       " 'ale',\n",
       " 'alez',\n",
       " 'ależ',\n",
       " 'ani',\n",
       " 'az',\n",
       " 'aż',\n",
       " 'bardziej',\n",
       " 'bardzo',\n",
       " 'beda',\n",
       " 'bede',\n",
       " 'bedzie',\n",
       " 'bez',\n",
       " 'bo',\n",
       " 'bowiem',\n",
       " 'by',\n",
       " 'byc',\n",
       " 'byl',\n",
       " 'byla',\n",
       " 'byli',\n",
       " 'bylo',\n",
       " 'byly',\n",
       " 'bym',\n",
       " 'bynajmniej',\n",
       " 'być',\n",
       " 'był',\n",
       " 'była',\n",
       " 'było',\n",
       " 'były',\n",
       " 'będzie',\n",
       " 'będą',\n",
       " 'będę',\n",
       " 'cala',\n",
       " 'cali',\n",
       " 'caly',\n",
       " 'cała',\n",
       " 'cały',\n",
       " 'chce',\n",
       " 'choć',\n",
       " 'ci',\n",
       " 'cie',\n",
       " 'ciebie',\n",
       " 'cię',\n",
       " 'co',\n",
       " 'cokolwiek',\n",
       " 'coraz',\n",
       " 'cos',\n",
       " 'coś',\n",
       " 'czasami',\n",
       " 'czasem',\n",
       " 'czemu',\n",
       " 'czy',\n",
       " 'czyli',\n",
       " 'często',\n",
       " 'daleko',\n",
       " 'dla',\n",
       " 'dlaczego',\n",
       " 'dlatego',\n",
       " 'do',\n",
       " 'dobrze',\n",
       " 'dokad',\n",
       " 'dokąd',\n",
       " 'dosc',\n",
       " 'dość',\n",
       " 'duzo',\n",
       " 'dużo',\n",
       " 'dwa',\n",
       " 'dwaj',\n",
       " 'dwie',\n",
       " 'dwoje',\n",
       " 'dzis',\n",
       " 'dzisiaj',\n",
       " 'dziś',\n",
       " 'gdy',\n",
       " 'gdyby',\n",
       " 'gdyz',\n",
       " 'gdyż',\n",
       " 'gdzie',\n",
       " 'gdziekolwiek',\n",
       " 'gdzies',\n",
       " 'gdzieś',\n",
       " 'go',\n",
       " 'godz',\n",
       " 'i',\n",
       " 'ich',\n",
       " 'ile',\n",
       " 'im',\n",
       " 'inna',\n",
       " 'inne',\n",
       " 'inny',\n",
       " 'innych',\n",
       " 'iv',\n",
       " 'ix',\n",
       " 'iz',\n",
       " 'iż',\n",
       " 'ja',\n",
       " 'jak',\n",
       " 'jakas',\n",
       " 'jakaś',\n",
       " 'jakby',\n",
       " 'jaki',\n",
       " 'jakichs',\n",
       " 'jakichś',\n",
       " 'jakie',\n",
       " 'jakis',\n",
       " 'jakiz',\n",
       " 'jakiś',\n",
       " 'jakiż',\n",
       " 'jakkolwiek',\n",
       " 'jako',\n",
       " 'jakos',\n",
       " 'jakoś',\n",
       " 'je',\n",
       " 'jeden',\n",
       " 'jedna',\n",
       " 'jednak',\n",
       " 'jednakze',\n",
       " 'jednakże',\n",
       " 'jedno',\n",
       " 'jednym',\n",
       " 'jedynie',\n",
       " 'jego',\n",
       " 'jej',\n",
       " 'jemu',\n",
       " 'jesli',\n",
       " 'jest',\n",
       " 'jestem',\n",
       " 'jeszcze',\n",
       " 'jezeli',\n",
       " 'jeśli',\n",
       " 'jeżeli',\n",
       " 'juz',\n",
       " 'już',\n",
       " 'ją',\n",
       " 'kazdy',\n",
       " 'każdy',\n",
       " 'kiedy',\n",
       " 'kierunku',\n",
       " 'kilka',\n",
       " 'kilku',\n",
       " 'kims',\n",
       " 'kimś',\n",
       " 'kto',\n",
       " 'ktokolwiek',\n",
       " 'ktora',\n",
       " 'ktore',\n",
       " 'ktorego',\n",
       " 'ktorej',\n",
       " 'ktory',\n",
       " 'ktorych',\n",
       " 'ktorym',\n",
       " 'ktorzy',\n",
       " 'ktos',\n",
       " 'ktoś',\n",
       " 'która',\n",
       " 'które',\n",
       " 'którego',\n",
       " 'której',\n",
       " 'który',\n",
       " 'których',\n",
       " 'którym',\n",
       " 'którzy',\n",
       " 'ku',\n",
       " 'lecz',\n",
       " 'lub',\n",
       " 'ma',\n",
       " 'mają',\n",
       " 'mam',\n",
       " 'mamy',\n",
       " 'mało',\n",
       " 'mi',\n",
       " 'miał',\n",
       " 'miedzy',\n",
       " 'mimo',\n",
       " 'między',\n",
       " 'mna',\n",
       " 'mnie',\n",
       " 'mną',\n",
       " 'moga',\n",
       " 'mogą',\n",
       " 'moi',\n",
       " 'moim',\n",
       " 'moj',\n",
       " 'moja',\n",
       " 'moje',\n",
       " 'moze',\n",
       " 'mozliwe',\n",
       " 'mozna',\n",
       " 'może',\n",
       " 'możliwe',\n",
       " 'można',\n",
       " 'mu',\n",
       " 'musi',\n",
       " 'my',\n",
       " 'mój',\n",
       " 'na',\n",
       " 'nad',\n",
       " 'nam',\n",
       " 'nami',\n",
       " 'nas',\n",
       " 'nasi',\n",
       " 'nasz',\n",
       " 'nasza',\n",
       " 'nasze',\n",
       " 'naszego',\n",
       " 'naszych',\n",
       " 'natomiast',\n",
       " 'natychmiast',\n",
       " 'nawet',\n",
       " 'nia',\n",
       " 'nic',\n",
       " 'nich',\n",
       " 'nie',\n",
       " 'niech',\n",
       " 'niego',\n",
       " 'niej',\n",
       " 'niemu',\n",
       " 'nigdy',\n",
       " 'nim',\n",
       " 'nimi',\n",
       " 'niz',\n",
       " 'nią',\n",
       " 'niż',\n",
       " 'no',\n",
       " 'o',\n",
       " 'obok',\n",
       " 'od',\n",
       " 'ok',\n",
       " 'około',\n",
       " 'on',\n",
       " 'ona',\n",
       " 'one',\n",
       " 'oni',\n",
       " 'ono',\n",
       " 'oraz',\n",
       " 'oto',\n",
       " 'owszem',\n",
       " 'pan',\n",
       " 'pana',\n",
       " 'pani',\n",
       " 'po',\n",
       " 'pod',\n",
       " 'podczas',\n",
       " 'pomimo',\n",
       " 'ponad',\n",
       " 'poniewaz',\n",
       " 'ponieważ',\n",
       " 'powinien',\n",
       " 'powinna',\n",
       " 'powinni',\n",
       " 'powinno',\n",
       " 'poza',\n",
       " 'prawie',\n",
       " 'przeciez',\n",
       " 'przecież',\n",
       " 'przed',\n",
       " 'przede',\n",
       " 'przedtem',\n",
       " 'przez',\n",
       " 'przy',\n",
       " 'raz',\n",
       " 'razie',\n",
       " 'roku',\n",
       " 'rowniez',\n",
       " 'również',\n",
       " 'sam',\n",
       " 'sama',\n",
       " 'sie',\n",
       " 'się',\n",
       " 'skad',\n",
       " 'skąd',\n",
       " 'soba',\n",
       " 'sobie',\n",
       " 'sobą',\n",
       " 'sposob',\n",
       " 'sposób',\n",
       " 'swoje',\n",
       " 'są',\n",
       " 'ta',\n",
       " 'tak',\n",
       " 'taka',\n",
       " 'taki',\n",
       " 'takich',\n",
       " 'takie',\n",
       " 'takze',\n",
       " 'także',\n",
       " 'tam',\n",
       " 'te',\n",
       " 'tego',\n",
       " 'tej',\n",
       " 'tel',\n",
       " 'temu',\n",
       " 'ten',\n",
       " 'teraz',\n",
       " 'też',\n",
       " 'to',\n",
       " 'toba',\n",
       " 'tobie',\n",
       " 'tobą',\n",
       " 'totez',\n",
       " 'toteż',\n",
       " 'totobą',\n",
       " 'trzeba',\n",
       " 'tu',\n",
       " 'tutaj',\n",
       " 'twoi',\n",
       " 'twoim',\n",
       " 'twoj',\n",
       " 'twoja',\n",
       " 'twoje',\n",
       " 'twym',\n",
       " 'twój',\n",
       " 'ty',\n",
       " 'tych',\n",
       " 'tylko',\n",
       " 'tym',\n",
       " 'tys',\n",
       " 'tzw',\n",
       " 'tę',\n",
       " 'u',\n",
       " 'vi',\n",
       " 'vii',\n",
       " 'viii',\n",
       " 'w',\n",
       " 'wam',\n",
       " 'wami',\n",
       " 'was',\n",
       " 'wasi',\n",
       " 'wasz',\n",
       " 'wasza',\n",
       " 'wasze',\n",
       " 'we',\n",
       " 'według',\n",
       " 'wie',\n",
       " 'wiele',\n",
       " 'wielu',\n",
       " 'więc',\n",
       " 'więcej',\n",
       " 'wlasnie',\n",
       " 'wszyscy',\n",
       " 'wszystkich',\n",
       " 'wszystkie',\n",
       " 'wszystkim',\n",
       " 'wszystko',\n",
       " 'wtedy',\n",
       " 'wy',\n",
       " 'właśnie',\n",
       " 'wśród',\n",
       " 'xi',\n",
       " 'xii',\n",
       " 'xiii',\n",
       " 'xiv',\n",
       " 'xv',\n",
       " 'z',\n",
       " 'za',\n",
       " 'zaden',\n",
       " 'zadna',\n",
       " 'zadne',\n",
       " 'zadnych',\n",
       " 'zapewne',\n",
       " 'zawsze',\n",
       " 'zaś',\n",
       " 'ze',\n",
       " 'zeby',\n",
       " 'znow',\n",
       " 'znowu',\n",
       " 'znów',\n",
       " 'zostal',\n",
       " 'został',\n",
       " 'żaden',\n",
       " 'żadna',\n",
       " 'żadne',\n",
       " 'żadnych',\n",
       " 'że',\n",
       " 'żeby'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tam jest biały pies\n"
     ]
    }
   ],
   "source": [
    "zdanie = nlp(\"Tam jest biały pies\")\n",
    "print(zdanie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tam tam ADV\n",
      "jest być AUX\n",
      "biały biały ADJ\n",
      "pies pies NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in zdanie:\n",
    "    print(token.text, token.lemma_, token.pos_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tam', 'jest', 'biały', 'pies']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in zdanie] #same tokeny "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokeny (słowa): ['To', 'jest', 'samochód', '.', 'Samochód', 'jest', 'niebieski', '.']\n",
      "Tokeny (zdania): ['To jest samochód.', 'Samochód jest niebieski.']\n"
     ]
    }
   ],
   "source": [
    "#tokenizacja ze względu na slowa i zdania \n",
    "\n",
    "doc = nlp(\"To jest samochód. Samochód jest niebieski.\")\n",
    "\n",
    "words = [token.text for token in doc]\n",
    "\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Tokeny (słowa):\", words)\n",
    "print(\"Tokeny (zdania):\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"pl\" id=\"385619847fae4bd6b4dcdb2be9f75280-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Tam</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">jest</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">biały</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">pies</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-385619847fae4bd6b4dcdb2be9f75280-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-385619847fae4bd6b4dcdb2be9f75280-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-385619847fae4bd6b4dcdb2be9f75280-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-385619847fae4bd6b4dcdb2be9f75280-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-385619847fae4bd6b4dcdb2be9f75280-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-385619847fae4bd6b4dcdb2be9f75280-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(zdanie, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"pl\" id=\"a416d31d705c40eab836491e64a9078b-0\" class=\"displacy\" width=\"650\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Tam</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">jest</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">biały</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">pies</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a416d31d705c40eab836491e64a9078b-0-0\" stroke-width=\"2px\" d=\"M62,152.0 62,127.0 197.0,127.0 197.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a416d31d705c40eab836491e64a9078b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,154.0 L58,146.0 66,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a416d31d705c40eab836491e64a9078b-0-1\" stroke-width=\"2px\" d=\"M212,152.0 212,102.0 500.0,102.0 500.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a416d31d705c40eab836491e64a9078b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,154.0 L208,146.0 216,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a416d31d705c40eab836491e64a9078b-0-2\" stroke-width=\"2px\" d=\"M362,152.0 362,127.0 497.0,127.0 497.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a416d31d705c40eab836491e64a9078b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M362,154.0 L358,146.0 366,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(zdanie, style = 'dep', options = {\"compact\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michał Żak persName\n",
      "Krakowie placeName\n",
      "7 września 2002 date\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Nazywam się Michał Żak. \\\n",
    "        Mam 28 lat i urodziłem się w Krakowie 7 września 2002.\") \n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Pobieranie danych z internetu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tekst piosenki:\\nStrange days have found us\\nStrange days have tracked us down\\nThey're goin' to destroy \\nOur casual joys\\nWe shall go on playing or find a new town\\nYeah\\n\\nStrange eyes fill strange rooms\\nVoices will signal their tired end\\nThe hostess is grinning\\nHer guests sleep from sinning\\nHear me talk of sin and you know this is it\\nYeah\\n\\nStrange days have found us\\nAnd through their strange hours, we linger alone\\nBodies confused\\nMemories misused\\nAs we run from the day to a strange night of stone\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\nDodaj adnotację do tego tekstu »\\nHistoria edycji tekstu\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def tekstowo(tytul, autor):\n",
    "    url = f\"https://www.tekstowo.pl/piosenka,{autor},{tytul}.html\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    lyrics = soup.find(\"div\", class_=\"song-text\").text.strip()\n",
    "    return lyrics\n",
    "\n",
    "        \n",
    "tekstowo(\"strange_days\",\"the_doors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recenzja 1: \"Wierzę w Amerykę\". Tak zaczyna się jedno z najważniejszych dzieł kinematografii, film wielokrotnie nagradzany, dziś już kultowy i stanowiący podmiot analizy wielu kinomaniaków. Kopalnia cytatów,... więcej\n",
      "\n",
      "Recenzja 2: \"Ojca chrzestnego\" Francisa Forda Coppoli widział lub chociaż słyszał o nim każdy szanujący się miłośnik kina. Minęło już niemal trzydzieści lat od premiery filmu twórcy \"Czasu apokalipsy\", a on... więcej\n",
      "\n",
      "Recenzja 3: Film F. F. Coppoli \"Ojciec chrzestny\", zwycięzca wielu nagród, w tym Oscara za najlepszy film i scenariusz adaptowany, jest uznawany przez krytyków i opinię publiczną za jedno z największych arcydzieł... więcej\n",
      "\n",
      "Recenzja 4: Ciężko jest mi pisać o prawdopodobnie najlepszym filmie w historii kina, jednak spróbuję. Oczywiście moje wypociny nie oddadzą prawdziwej jakości tego dzieła sztuki, ale mogą zachęcić do jego... więcej\n",
      "\n",
      "Recenzja 5: Lata 70. XX wieku z pewnością były jednymi z najważniejszych okresów w historii kina. To w tym czasie powstały klasyczne, niezapomniane \"Gwiezdne wojny\", niezbyt inteligentny, niezbyt przystojny, ale... więcej\n",
      "\n",
      "Recenzja 6: Mafijna rodzina Corleone, której losy były niezwykle zawikłane, niegdyś była u szczytu władzy, z czasem jednak ich potęga i wpływy u innych klanów znacznie się osłabiły. Rodem Corleone rządzi Don Vito... więcej\n",
      "\n",
      "Recenzja 7: \"Ojciec chrzestny\" to bez wątpienia jeden z najlepszych filmów, jakie kiedykolwiek obejrzałem. Adaptacja świetnej książki Mario Puzo zyskała status kultu. Rok 1945, Nowy Jork. Weteran drugiej wojny... więcej\n",
      "\n",
      "Recenzja 8: Ten film stał się jednym z głównych powodów, dla których nie lubię za dużo wiedzieć o filmie przed jego obejrzeniem. Czemu? Moje oczekiwania mogą przerosnąć rzeczywistość. I tak się właśnie stało w... więcej\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def pobierz_recenzje_filmweb(film_id):\n",
    "    url = f\"https://www.filmweb.pl/film/{film_id}/reviews\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    recenzje = []\n",
    "    for recenzja in soup.find_all('div', class_='flatReview__text'):\n",
    "        recenzje.append(recenzja.text.strip())\n",
    "        \n",
    "    return recenzje\n",
    "\n",
    "film_id = \"Ojciec+chrzestny-1972-1089\"  \n",
    "recenzje = pobierz_recenzje_filmweb(film_id)\n",
    "if recenzje:\n",
    "    for idx, recenzja in enumerate(recenzje, start=1):\n",
    "        print(f\"Recenzja {idx}: {recenzja}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie9\n",
    "    \n",
    "- Z użyciem bibliotek spacy/nltk przeanalizuj teksty: Ogniem i mieczem oraz Tajemnicza wyspa. \n",
    "    \n",
    "- Jakie top 10 słów niebędących stopwordsami występuje w obu lekturach najczęściej (podaj częstość, można zwizualizować)\n",
    "    \n",
    "- Jakie top 10 tagów występuje w obu lekturach najczęściej (podaj częstości)\n",
    "    \n",
    "   \n",
    "    \n",
    "Link do lektur:\n",
    "    \n",
    "https://clarin-pl.eu/dspace/handle/11321/110\n",
    "    \n",
    "https://clarin-pl.eu/dspace/handle/11321/465"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie10: Wejdź na dowolną listę przebojów (np. https://www.rmfmaxx.pl/hopbec) a następnie pobierz teksty piosenek z kolejnych pozycji. \n",
    "\n",
    "- Zamień wszystkie wyrazy na rozpoczynające się z małej litery. \n",
    "- Dokonaj tokenizacji. \n",
    "- Wyznacz miarę Herdana dla każdego z tekstów.\n",
    "- Narysuj zależność pomiędzy zajmowanym na liście miejscem a miarą Herdana. Wyznacz współczynnik korelacji. Czy występuje jakaś tendencja?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
