{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5566a9",
   "metadata": {},
   "source": [
    "<h3> Tokenizator OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc4481",
   "metadata": {},
   "source": [
    "https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d473f",
   "metadata": {},
   "source": [
    "<h3> Generowanie tekstu - znak po znaku (za pomocą GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e78f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd940ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bioinformatyka to interdyscyplinarna dziedzina nauki, która łączy biologię, informatykę i matematykę w celu analizy i interpretacji danych biologicznych.']\n",
      "\n",
      "[Iter 100] Loss value: 2.9913\n",
      "[Iter 200] Loss value: 2.4986\n",
      "[Iter 300] Loss value: 1.7353\n",
      "[Iter 400] Loss value: 1.1184\n",
      "[Iter 500] Loss value: 0.6791\n",
      "[Iter 600] Loss value: 0.3876\n",
      "[Iter 700] Loss value: 0.2260\n",
      "[Iter 800] Loss value: 0.1474\n",
      "[Iter 900] Loss value: 0.1034\n",
      "[Iter 1000] Loss value: 0.0770\n",
      "[Iter 1100] Loss value: 0.0597\n",
      "[Iter 1200] Loss value: 0.0478\n",
      "[Iter 1300] Loss value: 0.0393\n",
      "[Iter 1400] Loss value: 0.0328\n",
      "[Iter 1500] Loss value: 0.0279\n",
      "[Iter 1600] Loss value: 0.0240\n",
      "[Iter 1700] Loss value: 0.0208\n",
      "[Iter 1800] Loss value: 0.0182\n",
      "[Iter 1900] Loss value: 0.0161\n",
      "[Iter 2000] Loss value: 0.0143\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Bioinformatyka to interdyscyplinarna dziedzina nauki, która łączy biologię, informatykę i matematykę w celu analizy i interpretacji danych biologicznych.\"] \n",
    "         \n",
    "print(texts)\n",
    "print()\n",
    "\n",
    "# Model: \n",
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, v_size, hidden_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.ident = torch.eye(v_size) #macierz wektorow one-hot encoding dla wszytkich znakow\n",
    "        self.gru = nn.GRU(v_size, hidden_size, n_layers, batch_first=True) #uzyjemy sobie GRU\n",
    "        self.decoder = nn.Linear(hidden_size, v_size) #jako dekoder wezmiemy przeksztalcenie liniowe\n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        inp = self.ident[inp]                  #one-hot vector dla kolejnego znaku\n",
    "        output, hidden = self.gru(inp, hidden) #zastosowanie GRU\n",
    "        output = self.decoder(output)          #rozklad kolejnych znakow\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "txt = [\"<BOS>\"] + list(texts[0]) + [\"<EOS>\"]\n",
    "vocab = list(set(texts[0])) + [\"<BOS>\", \"<EOS>\"]\n",
    "vocab_size = len(vocab) #zapisuje te informacje pod zmienną\n",
    "vocab1 = {s: i for i, s in enumerate(vocab)}\n",
    "vocab2 = {i: s for i, s in enumerate(vocab)}\n",
    "\n",
    "model = TextGenerator(vocab_size, 20) #buduje model, z wymiarem dla stanu ukrytego = 16\n",
    "criterion = nn.CrossEntropyLoss() #funkcja kosztu\n",
    "\n",
    "\n",
    "txt_indices = [vocab1[ch] for ch in txt] #indeksy\n",
    "txt_tensor = torch.Tensor(txt_indices).long().unsqueeze(0) #przerabiamy na tensor\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #optymalizator\n",
    "criterion = nn.CrossEntropyLoss()                          #funkcja kosztu\n",
    "\n",
    "\n",
    "for it in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    input_seq = txt_tensor[:, :-1]    #wejscie <- bez ostatniego znaku\n",
    "    target = txt_tensor[:, 1:]        #target <- od 1 znaku\n",
    "    output, hidden = model(input_seq)\n",
    "    loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (it+1) % 100 == 0:\n",
    "        print(f\"[Iter {it+1}] Loss value: {float(loss):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311386ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def sample_sequence(model, max_len=100, temperature=0.8):\n",
    "    generated_sequence = \"\" #tu będzie przechowywana wygenerowana sekwencja\n",
    "    inp = torch.Tensor([vocab1[\"<BOS>\"]]).long() #zaczynamy od BOS\n",
    "    hidden = None\n",
    "    for p in range(max_len):\n",
    "        output, hidden = model(inp.unsqueeze(0), hidden) #co nam przeiwduje model?\n",
    "        output_dist = output.data.view(-1).div(temperature).exp() #uwzględniamy czynnik temperaturowy T, zamiast zi mamy zi/T--> normalnie\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0]) #indeks przewidywanego znaku\n",
    "        predicted_char = vocab2[top_i] #przewidywany znak\n",
    "        if predicted_char == \"<EOS>\": #jakim przewidywanym znakiem jest EOS to konczymy\n",
    "            break\n",
    "        generated_sequence += predicted_char #dodajemy kolejny znak\n",
    "        inp = torch.Tensor([top_i]).long() #zapisujemy indeks w postaci tensora\n",
    "        clear_output(wait=True) \n",
    "        print(generated_sequence)\n",
    "        time.sleep(0.1) \n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c33ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bioinformatyka to interdyscyplinarna dziedzina nauki, która łączy biologię, informatykę i matematykę\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(model, temperature=0.4)) #T=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2fb1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bioinforerpretacji danych biobiologicznaczywh.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(model, temperature=1.2)) #T=1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053e758",
   "metadata": {},
   "source": [
    "<h3> Modele Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebdac00",
   "metadata": {},
   "source": [
    "Tekst -> Tekst\n",
    "\n",
    "Gdzie?\n",
    "- Streszczenia\n",
    "- Tłumaczenia\n",
    "- Tekst piosenki -> Tytuł\n",
    "- Czaty, w tym automaty: Pytanie/Odpowiedź\n",
    "- i wiele innych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082d750",
   "metadata": {},
   "source": [
    "<h3> Prosty model do generowania podsumowania (bez sieci neuronowych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "208ed5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cały tekst:\n",
      "Bioinformatics is an interdisciplinary field that combines biology, computer science, mathematics, and statistics to analyze and interpret biological data. It has become an essential tool in modern biological research, particularly due to the rapid growth of genomic and molecular data generated by new technologies. The origins of bioinformatics can be traced back to the early days of genome sequencing, when scientists needed tools to store, search, and analyze DNA sequences. Today, bioinformatics is applied in a wide range of biological and medical fields, such as genomics, transcriptomics, proteomics, and systems biology. One of the most important applications of bioinformatics is in genomics, where it is used to assemble genomes, identify genes, and study genetic variations between individuals or species. Bioinformatics tools also help predict the structure and function of proteins, model biological pathways, and analyze evolutionary relationships. In medicine, bioinformatics plays a critical role in personalized medicine, where patients' genetic data can be analyzed to tailor drug treatments or predict disease risks. Bioinformatics methods are also used in cancer research, vaccine development, and drug discovery. A key feature of bioinformatics is its reliance on databases, algorithms, and software tools to process vast amounts of biological information. Tools like BLAST (for sequence comparison), multiple sequence alignment algorithms, and molecular modeling software are essential in many bioinformatics workflows. As the amount of biological data continues to grow exponentially, the demand for skilled bioinformaticians is also increasing. The field continues to evolve, incorporating machine learning, artificial intelligence, and big data technologies to gain new insights into complex biological systems. Bioinformatics not only helps scientists understand life at a molecular level but also bridges the gap between experimental data and practical applications in health, agriculture, and biotechnology. It is a dynamic and rapidly expanding discipline that plays a vital role in the future of science and medicine.\n",
      "----------------------------------------------------\n",
      "Podsumowanie:\n",
      "Bioinformatics is an interdisciplinary field that combines biology, computer science, mathematics, and statistics to analyze and interpret biological data. Bioinformatics tools also help predict the structure and function of proteins, model biological pathways, and analyze evolutionary relationships. A key feature of bioinformatics is its reliance on databases, algorithms, and software tools to process vast amounts of biological information.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"Cały tekst:\")\n",
    "text = \"Bioinformatics is an interdisciplinary field that combines biology, computer science, mathematics, and statistics to analyze and interpret biological data. It has become an essential tool in modern biological research, particularly due to the rapid growth of genomic and molecular data generated by new technologies. The origins of bioinformatics can be traced back to the early days of genome sequencing, when scientists needed tools to store, search, and analyze DNA sequences. Today, bioinformatics is applied in a wide range of biological and medical fields, such as genomics, transcriptomics, proteomics, and systems biology. One of the most important applications of bioinformatics is in genomics, where it is used to assemble genomes, identify genes, and study genetic variations between individuals or species. Bioinformatics tools also help predict the structure and function of proteins, model biological pathways, and analyze evolutionary relationships. In medicine, bioinformatics plays a critical role in personalized medicine, where patients' genetic data can be analyzed to tailor drug treatments or predict disease risks. Bioinformatics methods are also used in cancer research, vaccine development, and drug discovery. A key feature of bioinformatics is its reliance on databases, algorithms, and software tools to process vast amounts of biological information. Tools like BLAST (for sequence comparison), multiple sequence alignment algorithms, and molecular modeling software are essential in many bioinformatics workflows. As the amount of biological data continues to grow exponentially, the demand for skilled bioinformaticians is also increasing. The field continues to evolve, incorporating machine learning, artificial intelligence, and big data technologies to gain new insights into complex biological systems. Bioinformatics not only helps scientists understand life at a molecular level but also bridges the gap between experimental data and practical applications in health, agriculture, and biotechnology. It is a dynamic and rapidly expanding discipline that plays a vital role in the future of science and medicine.\"\n",
    "print(text)\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text.lower())\n",
    "\n",
    "#Zliczenia słów\n",
    "word_freq = Counter(words)\n",
    "\n",
    "#Wybór \"najlepszych\" zdań\n",
    "sentence_scores = {}\n",
    "for sent in sentences:\n",
    "    sentence_words = word_tokenize(sent.lower())\n",
    "    score = [word_freq[word] for word in sentence_words if word.isalnum() and word not in stop_words]\n",
    "    score = sum(score)/len(score)\n",
    "    sentence_scores[sent] = score\n",
    "\n",
    "#wybór top 3 zdań\n",
    "summary = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:3]\n",
    "\n",
    "print(\"Podsumowanie:\")\n",
    "print(\" \".join(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a2a97",
   "metadata": {},
   "source": [
    "<h3> Model do tłumaczenia oparty o sieci rekurencyjne (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2ea4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowe dane\n",
    "dane = [\n",
    "    (\"Czesc\", \"Hello\"),\n",
    "    (\"Jak sie masz\", \"How are you\"),\n",
    "    (\"Mam na imie Anna\", \"My name is Anna\"),\n",
    "    (\"To jest moj kot\", \"This is my cat\"),\n",
    "    (\"Lubie kawe\", \"I like coffee\"),\n",
    "    (\"Dzisiaj jest zimno\", \"It is cold today\"),\n",
    "    (\"Idziemy do sklepu\", \"We are going to the store\"),\n",
    "    (\"On jest moim bratem\", \"He is my brother\"),\n",
    "    (\"To bardzo dobre\", \"This is very good\"),\n",
    "    (\"Nie rozumiem\", \"I do not understand\"),\n",
    "    (\"Mieszkam w Krakowie\", \"I live in Cracow\"),\n",
    "    (\"Chce sie uczyc angielskiego\", \"I want to learn English\"),\n",
    "    (\"To jest latwe\", \"This is easy\"),\n",
    "    (\"Zaraz wracam\", \"I will be right back\"),\n",
    "    (\"Jestem zmeczony\", \"I am tired\"),\n",
    "    (\"Czy mozesz mi pomoc\", \"Can you help me\"),\n",
    "    (\"Lubie czytac ksiazki\", \"I like reading books\"),\n",
    "    (\"O ktorej godzinie\", \"At what time\"),\n",
    "    (\"Dobrze cie widziec\", \"Good to see you\"),\n",
    "    (\"Nie mam czasu\", \"I do not have time\"),\n",
    "    (\"To jest moj dom\", \"This is my house\"),\n",
    "    (\"Nie wiem\", \"I do not know\"),\n",
    "    (\"Co to znaczy\", \"What does it mean\"),\n",
    "    (\"Uwazaj\", \"Be careful\"),\n",
    "    (\"Jestem glodny\", \"I am hungry\"),\n",
    "    (\"Pojdzmy na spacer\", \"Let us go for a walk\"),\n",
    "    (\"To jest moj przyjaciel\", \"This is my friend\"),\n",
    "    (\"Ucze sie polskiego\", \"I am learning Polish\"),\n",
    "    (\"Potrzebuje pomocy\", \"I need help\"),\n",
    "    (\"To nie ma sensu\", \"That does not make sense\")\n",
    "]\n",
    "# Budujemy słownik\n",
    "def build_vocab(dane):\n",
    "    vocab = {\"<PAD>\": 0, \"<SOS>\":1, \"<EOS>\":2}\n",
    "    idx = 3\n",
    "    for src, tgt in dane:\n",
    "        for word in word_tokenize(src.lower()) + word_tokenize(tgt.lower()):\n",
    "            if word not in vocab:\n",
    "                vocab[word] = idx\n",
    "                idx += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(dane)\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def encode(sentence, vocab, sos=False, eos=False):\n",
    "    ids = [vocab[word] for word in word_tokenize(sentence.lower())]\n",
    "    if sos: ids = [vocab[\"<SOS>\"]] + ids\n",
    "    if eos: ids = ids + [vocab[\"<EOS>\"]]\n",
    "    return torch.tensor(ids)\n",
    "\n",
    "#dane\n",
    "pairs = [(encode(src, vocab), encode(tgt, vocab, sos=True, eos=True)) for src, tgt in dane]\n",
    "\n",
    "#parametry\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e412ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 127.80132937431335\n",
      "Epoch 10, Loss: 0.19135316263418645\n",
      "Epoch 20, Loss: 0.06985790480393916\n",
      "Epoch 30, Loss: 0.03791447635740042\n",
      "Epoch 40, Loss: 0.023940994375152513\n",
      "Epoch 50, Loss: 0.016431700496468693\n",
      "Epoch 60, Loss: 0.01189034795970656\n",
      "Epoch 70, Loss: 0.00892099273914937\n",
      "Epoch 80, Loss: 0.00687036981980782\n",
      "Epoch 90, Loss: 0.00539706524432404\n"
     ]
    }
   ],
   "source": [
    "#Enkoder -> cel: zakodowanie zdania wejsciowego\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, hidden = self.gru(x)\n",
    "        return hidden\n",
    "\n",
    "#Dekoder -> cel: rozkodowanie zdania wyjsciowego\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "enc_optim = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "dec_optim = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "#Pętla ucząca\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for src, tgt in pairs:\n",
    "        src = src.unsqueeze(0)  # batch=1\n",
    "        tgt_input = tgt[:-1].unsqueeze(0)\n",
    "        tgt_target = tgt[1:].unsqueeze(0)\n",
    "\n",
    "        hidden = encoder(src)\n",
    "        output, _ = decoder(tgt_input, hidden)\n",
    "\n",
    "        loss = criterion(output.squeeze(0), tgt_target.squeeze(0))\n",
    "        \n",
    "        enc_optim.zero_grad()\n",
    "        dec_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ce689",
   "metadata": {},
   "source": [
    "* Czy to działa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ff27056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Czy to działa?\n",
    "def generate(tekst):\n",
    "    src = encode(tekst, vocab).unsqueeze(0)\n",
    "    hidden = encoder(src)\n",
    "    \n",
    "    input_token = torch.tensor([[vocab[\"<SOS>\"]]])\n",
    "    result = []\n",
    "\n",
    "    for _ in range(15):\n",
    "        output, hidden = decoder(input_token, hidden)\n",
    "        pred_id = output.argmax(2).item() #wybieram najbardziej prawdopodobne slowo\n",
    "        if pred_id == vocab[\"<EOS>\"]:\n",
    "            break\n",
    "        result.append(inv_vocab[pred_id])\n",
    "        input_token = torch.tensor([[pred_id]])\n",
    "        clear_output(wait=True) \n",
    "        print(\" \".join(result))\n",
    "        time.sleep(0.3) \n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e758cbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i will be right back\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Zaraz wracam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb63815",
   "metadata": {},
   "source": [
    "Usprawnienia:\n",
    "- rozbudowa sieci\n",
    "- mechanizm atencji\n",
    "- teacher forcing\n",
    "\n",
    "Warto poczytać: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1eb7c0",
   "metadata": {},
   "source": [
    "<h4> Jak ocenić wygenerowany tekst?\n",
    "    \n",
    "- ROUGE (najczęściej przy streszczeniach,  oparty o n-gramy)\n",
    "- BLEU (najczęściej do tłumaczeń, dosyć restrykcyjny, oparty o n-gramy)\n",
    "    \n",
    "Istnieją też bardziej zaawansowane miary, jak BERTScore, ten ocenia semantyczne podobieństwo odpowiedzi z użyciem embeddingów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02fd280",
   "metadata": {},
   "source": [
    "https://lightning.ai/docs/torchmetrics/stable/text/rouge_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2359983",
   "metadata": {},
   "source": [
    "https://pub.aimind.so/unveiling-the-power-of-rouge-metrics-in-nlp-b6d3f96d3363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c30911be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.75, recall=0.6, fmeasure=0.6666666666666665), 'rouge2': Score(precision=0.3333333333333333, recall=0.25, fmeasure=0.28571428571428575), 'rougeL': Score(precision=0.5, recall=0.4, fmeasure=0.4444444444444445)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Przykładowe teksty\n",
    "reference = \"to jest sa przykladowe zdanie\"\n",
    "candidate = \"przykladowe to jest zdania\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True) #L - najdluzszy podciąg\n",
    "scores = scorer.score(reference, candidate)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e0e031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.16990442448471224\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "reference = [[\"to\", \"jest\", \"przykładowe\", \"zdanie\"]]\n",
    "candidate = [\"to\", \"jest\", \"przykład\", \"zdania\"]\n",
    "\n",
    "# Obliczanie BLEU\n",
    "bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1) #(ile z wygenerowanych n-gramow znajduje sie w referencji)\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e228a1",
   "metadata": {},
   "source": [
    "- Na następne zajęcia proszę wykonać jedno z poniższych zadań (do wyboru). W razie konieczności zmniejsz rozważany zbiór danych (dotyczy szczególnie zad2). Na Teams proszę zamieścić plik jupyter oraz pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127da81a",
   "metadata": {},
   "source": [
    "<h4> Zadanie1. Zbuduj (zgodnie z tutorialem) model do tłumaczenia tekstu z języka francuskiego na angielski. \n",
    "    \n",
    "- Dodatkowo, podziel na początku zbiór danych na treningowy, testowy, walidacyjny (zaproponuj w jakich proporcjach)\n",
    "- Zastosuj metrykę BLEU do tego zadania jako miarę oceniającą wygenerowane tłumaczenia\n",
    "- Potestuj kilka architektur sieci neuronowych, opcjonalnie możesz wykorzystać również GloVe\n",
    "- Opracuj w formie krótkiego sprawozdania (max 2 strony) najważniejsze kroki analizy oraz zamieść odpowiednie wykresy - spadek funkcji kosztu w kolejnych epokach (na zbiorze treningowym i walidacyjnym), wizualizacje atencji na przykładowych zdaniach, miara BLEU na zbiorze treningowym, walidacyjnym i testowym. Ile danych rozważano?\n",
    "    \n",
    "    \n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d04ba66",
   "metadata": {},
   "source": [
    "<h4> Zadanie2: Dokonaj dostrojenia modelu T5 (Text-to-Text Transfer Transformer) do zadania generowania podsumowań na zbiorze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0504f",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/knkarthick/samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acb5d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "splits = {'train': 'train.csv', 'validation': 'validation.csv', 'test': 'test.csv'}\n",
    "df = pd.read_csv(\"hf://datasets/knkarthick/samsum/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418bbe5a",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "- Opisz krótko model T5 (co to za model, jaka architektura, ile parametrów, na jakich tekstach trenowano)\n",
    "- Czego dotyczą te dane?\n",
    "- Porównaj słowa występujące w streszczeniu i pełnym tekście (w szczególności, jaki % ich się znajduje w pełnym tekście)\n",
    "- Narysuj rozkład długości tekstów pełnych i streszczeń\n",
    "- Rozważ tylko te najkrótsze teksty, jakie?\n",
    "- Zastosuj metrykę ROUGE do oceny modelu: przed treningiem i po (na rozważanych zbiorach)\n",
    "- Potestuj różne hiperparametry (szczególnie w TrainingArguments)\n",
    "- W krótkim sprawozdaniu (max 2 strony) opisz dokładnie wszystkie argumenty występujące w TrainingArguments oraz Trainer oraz zamieść wykresy funkcji kosztu podczas procesu uczenia oraz wyniki dotyczące metryki ROUGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3db749",
   "metadata": {},
   "source": [
    "Możesz oprzeć się na:\n",
    "    \n",
    "* https://huggingface.co/google-t5/t5-small\n",
    "* https://medium.com/@anyuanay/fine-tuning-the-pre-trained-t5-small-model-in-hugging-face-for-text-summarization-3d48eb3c4360\n",
    "* https://medium.com/@syahmisajid12/summarization-using-fine-tuned-t5-transformer-62d0630ff795  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fde81",
   "metadata": {},
   "source": [
    "T5 (Text-to-Text Transfer Transformer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e34ddf",
   "metadata": {},
   "source": [
    "<h3> To się może przydać (np do projektu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f4c47",
   "metadata": {},
   "source": [
    "*  ponad 5000 par pytanie–odpowiedź w języku polskim,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f86a7",
   "metadata": {},
   "source": [
    "https://www.futurebeeai.com/dataset/prompt-response-dataset/polish-closed-ended-question-answer-text-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa67578",
   "metadata": {},
   "source": [
    "* 7000 pytań i inne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a9387",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/ipipan/polqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8046e",
   "metadata": {},
   "source": [
    "- streszczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19106a2",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/allegro/summarization-polish-summaries-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fefcc7",
   "metadata": {},
   "source": [
    "- posiedzenia sejmu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4707c76",
   "metadata": {},
   "source": [
    "https://www.sejm.gov.pl/sejm7.nsf/poslowie.xsp?type=P#K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2680f",
   "metadata": {},
   "source": [
    "Mamy też Web Scraping, ale pamiętajmy! \n",
    "- jeżeli to są małe ilości, krótkie wypowiedzi (prawo cytatu) to raczej ok\n",
    "- zazwyczaj nie ma problemu ze źródłami z domen publicznych\n",
    "- w przypadku stron komercyjnych, duże ilości pobieranych danych -> często udostępniane są API (to może być płatne)\n",
    "- jeżeli byśmy chcieli wykorzystywać zgromadzone dane czy zbudowane modele komercyjne, to sytucja bardziej skomplikowana :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975dd698",
   "metadata": {},
   "source": [
    "<h3> Inne, warte rozważenia zbiory danych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b909be3",
   "metadata": {},
   "source": [
    "https://github.com/allenai/gooaq #pytania/odp na podstawie sugestii google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af6dcd",
   "metadata": {},
   "source": [
    "https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c79ce",
   "metadata": {},
   "source": [
    "<h3> Speakleash (spichlerz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be944498",
   "metadata": {},
   "source": [
    "https://pypi.org/project/speakleash/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823f8d9",
   "metadata": {},
   "source": [
    "https://github.com/speakleash/speakleash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5355bb8",
   "metadata": {},
   "source": [
    "- narzędzie w Pythonie służące do zarządzania dużymi zbiorami danych tekstowych w języku polskim, wykorzystywane m.in. do trenowania modeli językowych\n",
    "- obszerny zestaw metryk + klasyfikacja tematyczna dokumentu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d2c816",
   "metadata": {},
   "source": [
    "<h3> Bielik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223fd4b8",
   "metadata": {},
   "source": [
    "- duży polskojęzyczny model językowy (LLM), miliardy parametrów\n",
    "- wytrenowany na danych z projektu SpeakLeash\n",
    "- oparty na architekturze Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9858cee9",
   "metadata": {},
   "source": [
    "https://bielik.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32bb0ed",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/speakleash/Bielik-7B-Instruct-v0.1 (starsza wersja)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39b473",
   "metadata": {},
   "source": [
    "https://huggingface.co/speakleash/Bielik-11B-v2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
